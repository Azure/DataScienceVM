{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSVM Tutorial\n",
    "\n",
    "This tutorial was created to showcase some of the features of the Ubuntu DSVM. It shows many steps of the data science process using the CIFAR-10 dataset with Keras+TensorFlow. CIFAR-10 is a popular dataset for image classification, collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. It contains 60,000 images of 10 different types of objects (truck, automobile, cat, etc.).\n",
    "\n",
    "This tutorial is divided into three parts:\n",
    "\n",
    "0. Configure AzureML. This notebook sets up AzureML and saves the necessary configuration info to a file for other notebooks.\n",
    "1. Load data. This notebook downloads the CIFAR-10 dataset and saves it to disk. It also prepares the dataset for remote runs on Azure by saving it to blob storage. \n",
    "2a. Train a model. This notebook trains a deep learning model to classify images as one of the CIFAR-10 categories (truck, cat, etc.). This is done locally.\n",
    "3. Deploy a model. This notebook shows you how to create a REST API with you model using AzureML.\n",
    "\n",
    "There are also three demo notebooks:\n",
    "2b. Train remotely. This notebook trains a model on a Batch AI cluster, leveraging the power of Batch AI to scale up and optionally scale out a distributed training job.\n",
    "2c. Hyperparameter sweep. This notebook leverages the HyperDrive service to explore the hyperparameter space and improve the model's performance.\n",
    "4. AutoML. This notebook demonstrates the simplicity and power of AutoML for automated machine learning.\n",
    "\n",
    "This tutorial was originally created for Microsoft's internal machine learning and data science conference (MLADS), but you can also run it on an Ubuntu DSVM of your own outside of the conference.\n",
    "\n",
    "### Part 1: Load data\n",
    "\n",
    "This tutorial will show how to prepare image data sets for use with deep learning algorithms in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as ShowImage\n",
    "ShowImage(url=\"https://cntk.ai/jup/201/cifar-10.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataset to disk\n",
    "\n",
    "Keras includes a convenient method to load the complete CIFAR-10 dataset into memory. Here we serialize it to disk so we can demonstrate how to upload it to blob storage for AzureML remote runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pc\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "data_dir = './data/cifar10'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "with open(os.path.join(data_dir, 'x_train'), 'wb') as f:\n",
    "    pc.dump(x_train, f)\n",
    "with open(os.path.join(data_dir, 'y_train'), 'wb') as f:\n",
    "    pc.dump(y_train, f)\n",
    "with open(os.path.join(data_dir, 'x_test'), 'wb') as f:\n",
    "    pc.dump(x_test, f)\n",
    "with open(os.path.join(data_dir, 'y_test'), 'wb') as f:\n",
    "    pc.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to the AzureML datastore\n",
    "\n",
    "AzureML supports running a Python file remotely on Azure to take advantage of GPU VM instances or simplified distributed training on Batch AI. You can easily develop the file on your DSVM, then scale on Azure as needed. We need to upload our dataset to blob storage so it is accessible by these remote runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data/cifar10', target_path='cifar10', overwrite=True, show_progress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
