{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSVM Tutorial\n",
    "\n",
    "This tutorial was created to showcase some of the features of the Ubuntu DSVM. It shows many steps of the data science process using the CIFAR-10 dataset. CIFAR-10 is a popular dataset for image classification, collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. It contains 60,000 images of 10 different types of objects (truck, automobile, cat, etc.).\n",
    "\n",
    "This tutorial is divided into three parts:\n",
    "\n",
    "1. Load data. This notebook downloads the CIFAR-10 dataset and processes it to convert it to the format expected by CNTK. This processing is parallelized with Spark, which is included on the DSVM for single-node tasks.\n",
    "2. Train a model. This notebook trains a basic deep learning model to classify images as one of the CIFAR-10 categories (truck, cat, etc.).\n",
    "3. Deploy a model. This notebook shows you how to create a REST API with you model using Microsoft ML Server.\n",
    "\n",
    "This tutorial was originally created for Microsoft's internal machine learning and data science conference (MLADS), but you can also run it on an Ubuntu DSVM of your own outside of the conference.\n",
    "\n",
    "### Part 1: Load data\n",
    "\n",
    "This tutorial will show how to prepare image data sets for use with deep learning algorithms in CNTK. The CIFAR-10 dataset is not included in the CNTK distribution but can be easily downloaded and converted to CNTK-supported format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cntk.ai/jup/201/cifar-10.png\" width=\"500\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as ShowImage\n",
    "ShowImage(url=\"https://cntk.ai/jup/201/cifar-10.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import shutil\n",
    "try: \n",
    "    from urllib.request import urlretrieve \n",
    "except ImportError: \n",
    "    from urllib import urlretrieve\n",
    "\n",
    "def downloadData(src):\n",
    "    print ('Downloading ' + src)\n",
    "    fname, h = urlretrieve(src, './delete.me')\n",
    "    try:\n",
    "        with tarfile.open(fname) as tar:\n",
    "            tar.extractall()\n",
    "        print ('Done.')\n",
    "    finally:\n",
    "        os.remove(fname)\n",
    "    \n",
    "# Paths for saving the text files\n",
    "data_dir = './data/CIFAR-10/'\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "try:\n",
    "    os.chdir(data_dir)   \n",
    "    \n",
    "    # use the dataset that was already downloaded by the setup script\n",
    "    # downloadData('http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz')\n",
    "    \n",
    "    shutil.copyfile('/data/cifar-10-python.tar.gz', 'cifar-10-python.tar.gz')\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "            tar.extractall()\n",
    "            \n",
    "finally:\n",
    "    os.chdir(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data with Spark\n",
    "\n",
    "The downloaded CIFAR-10 dataset contains six files, each with 10,000 images. Here we use five files for training and one for testing. \n",
    "\n",
    "The images can be processed in serial with a *for* loop, but that is slow even for CIFAR-10. Here we show how the standalone Spark instance can be used to process each file in parallel to take advantage of more cores on the VM.\n",
    "\n",
    "CNTK requires us to also provide a *mean* image, where each pixel value is the mean over all images in the training set, so images can be normalized during training. We compute the mean image by computing the *summed* image for each parallel batch, then averaging over the sums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as cp\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# CIFAR Image data\n",
    "imgSize = 32\n",
    "numFeature = imgSize * imgSize * 3\n",
    "\n",
    "foldername = 'train'\n",
    "\n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark job\n",
      "Spark job finished\n"
     ]
    }
   ],
   "source": [
    "# processes a batch of images in a single file\n",
    "def processBatch(ifile):    \n",
    "    mapFileArray = []\n",
    "    dataMean = np.zeros((3, imgSize, imgSize)) # mean is in CHW format.\n",
    "        \n",
    "    filename = os.path.join('./data/CIFAR-10/cifar-10-batches-py', 'data_batch_' + str(ifile))\n",
    "    with open(filename, 'rb') as f:\n",
    "                data = cp.load(f, encoding='latin1')\n",
    "                for i in range(10000):\n",
    "                    filename = os.path.join(os.path.abspath(foldername), ('%05d.png' % (i + (ifile - 1) * 10000)))\n",
    "                    image_data = data['data'][i, :]\n",
    "                    label = data['labels'][i]\n",
    "                    saveImage(filename, image_data, label, 4, mean=dataMean)\n",
    "                    \n",
    "                    ## add to mapFileArray\n",
    "                    mapFileArray.append(\"%s\\t%d\\n\" % (filename, label))\n",
    "    \n",
    "    return (mapFileArray, dataMean)                                    \n",
    "\n",
    "# saves a single image to a file\n",
    "def saveImage(filename, data, label, pad, **key_parms):    \n",
    "    # data in CIFAR-10 dataset is in CHW format.\n",
    "    pixData = data.reshape((3, imgSize, imgSize))\n",
    "    if ('mean' in key_parms):\n",
    "        key_parms['mean'] += pixData\n",
    "\n",
    "    if pad > 0:\n",
    "        pixData = np.pad(pixData, ((0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=128) \n",
    "\n",
    "    img = Image.new('RGB', (imgSize + 2 * pad, imgSize + 2 * pad))\n",
    "    pixels = img.load()\n",
    "    for x in range(img.size[0]):\n",
    "        for y in range(img.size[1]):\n",
    "            pixels[x, y] = (pixData[0][y][x], pixData[1][y][x], pixData[2][y][x])\n",
    "    img.save(filename)\n",
    "    \n",
    "SparkContext._ensure_initialized()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "    \n",
    "## do the parallel processing \n",
    "\n",
    "batch_file_names = []\n",
    "\n",
    "for ifile in range(1, 6):\n",
    "        batch_file_names.append(ifile)\n",
    "    \n",
    "print('Starting Spark job')\n",
    "image_rdd = spark.sparkContext.parallelize(batch_file_names).map(processBatch).collect()\n",
    "print('Spark job finished')\n",
    "\n",
    "## train_map.txt needs one line per image in the training set, so write that here\n",
    "\n",
    "with open('train_map.txt', 'w') as mapFile:\n",
    "    for row in image_rdd:\n",
    "        mapFile.writelines(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code to save the mean image\n",
    "\n",
    "import xml.etree.cElementTree as et\n",
    "import xml.dom.minidom\n",
    "\n",
    "def saveMean(fname, data):\n",
    "    root = et.Element('opencv_storage')\n",
    "    et.SubElement(root, 'Channel').text = '3'\n",
    "    et.SubElement(root, 'Row').text = str(imgSize)\n",
    "    et.SubElement(root, 'Col').text = str(imgSize)\n",
    "    meanImg = et.SubElement(root, 'MeanImg', type_id='opencv-matrix')\n",
    "    et.SubElement(meanImg, 'rows').text = '1'\n",
    "    et.SubElement(meanImg, 'cols').text = str(imgSize * imgSize * 3)\n",
    "    et.SubElement(meanImg, 'dt').text = 'f'\n",
    "    et.SubElement(meanImg, 'data').text = ' '.join(['%e' % n for n in np.reshape(data, (imgSize * imgSize * 3))])\n",
    "\n",
    "    tree = et.ElementTree(root)\n",
    "    tree.write(fname)\n",
    "    x = xml.dom.minidom.parse(fname)\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write(x.toprettyxml(indent = '  '))\n",
    "        \n",
    "dataMean = np.zeros((3, imgSize, imgSize)) # mean is in CHW format.\n",
    "for row in image_rdd:\n",
    "    dataMean += row[1]\n",
    "dataMean = dataMean / (50 * 1000)\n",
    "saveMean('CIFAR-10_mean.xml', dataMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set\n",
    "\n",
    "The test set only has one file with all 10,000 images in the download. We could parallelize the images within this file, but we skip that here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting test data to png images...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print ('Converting test data to png images...')\n",
    "    \n",
    "foldername = 'test'\n",
    "    \n",
    "if not os.path.exists(foldername):\n",
    "    os.makedirs(foldername)\n",
    "    \n",
    "mapItems = []\n",
    "    \n",
    "with open('test_map.txt', 'w') as mapFile:\n",
    "    with open(os.path.join('./data/CIFAR-10/cifar-10-batches-py', 'test_batch'), 'rb') as f:\n",
    "        data = cp.load(f, encoding='latin1')\n",
    "        for i in range(10000):\n",
    "            fname = os.path.join(os.path.abspath(foldername), ('%05d.png' % i))\n",
    "            saveImage(fname, data['data'][i, :], data['labels'][i], 0)\n",
    "            mapItems.append(\"%s\\t%d\\n\" % (fname, data['labels'][i]))\n",
    "               \n",
    "with open('test_map.txt', 'w') as mapFile:    \n",
    "    mapFile.writelines(mapItems)\n",
    "                \n",
    "print ('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
