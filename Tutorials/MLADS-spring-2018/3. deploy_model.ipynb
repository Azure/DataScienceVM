{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Deploy the model\n",
    "\n",
    "In the second notebook we created a basic model and exported it to a file. In this notebook we'll use that same model file to create a REST API with Microsoft ML Server. The Ubuntu DSVM has an installation of ML Server for testing deployments. We'll create a REST API with our model and test it with the same truck image we used in notebook 2 to evaluate the model. \n",
    "\n",
    "There are two variables you must set before running this notebook. The first is the password for your ML Server instance. At MLADS we've already set this for you. If you're following this tutorial on your own, you should configure your ML Server instance for [one-box deployment](https://docs.microsoft.com/en-us/machine-learning-server/operationalize/configure-machine-learning-server-one-box). The second variable is the name of the deployed web service. This needs to be unique on the VM. We recommend that you use your username and a number, like *username5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a unique service name. We recommend you use your username and a number, like alias3\n",
    "service_name = ____SET_ME_TO_A_UNIQUE_VALUE_____\n",
    "\n",
    "# set the ML Server admin password\n",
    "ml_server_password =  ____SET_ME_____ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microsoft ML Server Operationalization\n",
    "\n",
    "ML Server Operationalization provides the ability to easily convert a model into a REST API and call it from many languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://docs.microsoft.com/en-us/machine-learning-server/media/what-is-operationalization/data-scientist-easy-deploy.png\" width=\"800\" height=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as ShowImage\n",
    "ShowImage(url=\"https://docs.microsoft.com/en-us/machine-learning-server/media/what-is-operationalization/data-scientist-easy-deploy.png\", width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Server runs one or more web node as the front end for REST API calls and one more compute nodes to perform the calculations for the deployed services. This VM was configured for ML Server Operationalization when it was created. Here we run a single web node and single compute node on this VM in a *one-box* configuration.\n",
    "\n",
    "ML Server provides the azureml.deploy Python package to deploy new REST API endpoints and call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://docs.microsoft.com/en-us/machine-learning-server/operationalize/media/configure-machine-learning-server-one-box/setup-onebox.png\" width=\"800\" height=\"800\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image as ShowImage\n",
    "ShowImage(url=\"https://docs.microsoft.com/en-us/machine-learning-server/operationalize/media/configure-machine-learning-server-one-box/setup-onebox.png\", width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details are available in [the ML Server documentation](https://docs.microsoft.com/en-us/machine-learning-server/operationalize/configure-machine-learning-server-one-box-9-2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.deploy import DeployClient\n",
    "from azureml.deploy.server import MLServer\n",
    "\n",
    "HOST = 'http://localhost:12800'\n",
    "context = ('admin', ml_server_password)\n",
    "client = DeployClient(HOST, use=MLServer, auth=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the truck image for testing our deployed service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "from IPython.display import Image as ImageShow\n",
    "\n",
    "try: \n",
    "    from urllib.request import urlopen \n",
    "except ImportError: \n",
    "    from urllib import urlopen\n",
    "\n",
    "url = \"https://cntk.ai/jup/201/00014.png\"\n",
    "myimg = np.array(Image.open(urlopen(url)), dtype=np.float32)\n",
    "flattened = myimg.ravel()\n",
    "\n",
    "ImageShow(url=url, width=64, height=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "\n",
    "We need two functions to deploy a model in ML Server. The *init* function handles service initialization. The *eval* function evaluates a single input value and returns the result. *eval* will be called by the server when we call the REST API.\n",
    "\n",
    "Our *eval* function accepts a single input: a 1D numpy array with the image to evaluate. It needs to (1) reshape the input data from a 1D array to a 3D image, (2) subtract the image mean, to mimic the inputs to the model during training, (3) evaluate the model on the image, and (4) return the results as a pandas DataFrame. Alternatively we could return just the top result or the top three results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk\n",
    "\n",
    "with open('model.cntk', mode='rb') as file: # b is important -> binary\n",
    "    binary_model = file.read()\n",
    "\n",
    "# --Define an `init` function to handle service initialization --\n",
    "def init():\n",
    "    import cntk\n",
    "    global loaded_model\n",
    "    loaded_model = cntk.ops.functions.load_model(binary_model)\n",
    "    \n",
    "# define an eval function to handle scoring\n",
    "def eval(image_data):\n",
    "    import numpy as np\n",
    "    import cntk\n",
    "    from pandas import DataFrame\n",
    "    \n",
    "    image_data = image_data.copy().reshape((32, 32, 3))\n",
    "    \n",
    "    image_mean = 133.0\n",
    "    image_data -= image_mean\n",
    "    image_data = np.ascontiguousarray(np.transpose(image_data, (2, 0, 1)))\n",
    "    \n",
    "    results = loaded_model.eval({loaded_model.arguments[0]:[image_data]})\n",
    "        \n",
    "    return DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the API\n",
    "service = client.service(service_name)\\\n",
    "        .version('1.0')\\\n",
    "        .code_fn(eval, init)\\\n",
    "        .inputs(image_data=np.array)\\\n",
    "        .outputs(results=pd.DataFrame)\\\n",
    "        .models(binary_model=binary_model)\\\n",
    "        .description('My CNTK model')\\\n",
    "        .deploy()\n",
    "        \n",
    "print(help(service))\n",
    "service.capabilities()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call our newly created API with our truck image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = service.eval(flattened)\n",
    "\n",
    "# -- Pluck out the named output `results` as defined during publishing and print --\n",
    "print(res.output('results'))\n",
    "\n",
    "# get the top 3 predictions\n",
    "result = res.output('results')\n",
    "result = result.as_matrix()[0]\n",
    "top_count = 3\n",
    "result_indices = (-np.array(result)).argsort()[:top_count]\n",
    "\n",
    "label_lookup = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "print(\"Top 3 predictions:\")\n",
    "for i in range(top_count):\n",
    "    print(\"\\tLabel: {:10s}, confidence: {:.2f}%\".format(label_lookup[result_indices[i]], result[result_indices[i]] * 100))\n",
    "\n",
    "# -- Retrieve the URL of the swagger file for this service.\n",
    "cap = service.capabilities()\n",
    "swagger_URL = cap['swagger']\n",
    "print(swagger_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.swagger())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Microsoft ML Server (Python 3.5)",
   "language": "python",
   "name": "python3-mls"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
