{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning for Audio Part 2a - Pre-process UrbanSound Datset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook, we will process the audio files and extract the useful features that will be fed into a Convolutional Neural Network. \n",
    "\n",
    "\n",
    "\n",
    "We will train and predict on [UrbanSound8K](https://serv.cusp.nyu.edu/projects/urbansounddataset/download-urbansound8k.html) dataset. There are a few published benchmarks, which are mentioned in the papers below:\n",
    "\n",
    "- [Environmental sound classification with convolutional neural networks](http://karol.piczak.com/papers/Piczak2015-ESC-ConvNet.pdf) by Karol J Piczak.\n",
    "- [Deep convolutional neural networks and data augmentation for environmental sound classification](https://arxiv.org/abs/1608.04363) by Justin Salamon and Juan Pablo Bello\n",
    "- [Learning from Between-class Examples for Deep Sound Recognition](https://arxiv.org/abs/1711.10282) by Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada\n",
    "\n",
    "\n",
    "The state-of-art result is from the last paper by Tokozume et al., where the best error rate achieved is 21.7%. In this tutorial we will show you how to build a neural network that can achieve the state-of-art performance using Azure.\n",
    "\n",
    "\n",
    "This jupyter notebook borrows some of the pre-processing code on the Github Repo here: http://aqibsaeed.github.io/2016-09-24-urban-sound-classification-part-2/, but with a lot of modifications. It is tested with **Python3.5**, **Keras 2.1.2** and **Tensorflow 1.4.0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use librosa as our audio processing library. For more details on librosa, please refer to the librosa documenent [here](https://librosa.github.io/librosa/tutorial.html). We also need to install a bunch of libraries. Most of them are python packages, but you still may need to install a few audio processing libraries using apt-get:\n",
    "\n",
    "`sudo apt-get install -y --no-install-recommends \\\n",
    "        openmpi-bin \\\n",
    "        build-essential \\\n",
    "        autoconf \\\n",
    "        libtool \\\n",
    "        libav-tools \\\n",
    "        pkg-config`\n",
    "        \n",
    "        \n",
    "We also need to install librosa and a few other deep learning libraries in pip:\n",
    "\n",
    "`pip install librosa pydot graphviz keras tensorflow-gpu`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to licensing issues, we cannot download the data directly. Please go to the [UrbanSound8K Download](https://serv.cusp.nyu.edu/projects/urbansounddataset/download-urbansound8k.html) site, fill in the related information, download from there, and put it in the right place. You need to update the `parent_path` and `save_dir` below. In this particular case, we don't need the label file, as the labels are already reflected in the file names. We will parse the labels directly from the file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and initialize global varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "# used to featurize the dataset\n",
    "from scipy import signal\n",
    "\n",
    "# how many classes do we have; for one-hot encoding and parallel processing purpose\n",
    "num_total_classes = 10\n",
    "\n",
    "# Where you have saved the UrbanSound8K data set. Need to be absolute path.\n",
    "parent_dir = \"/mnt/UrbanSound8K/audio\"\n",
    "\n",
    "# specify bands that you want to use. This is also the \"height\" of the spectrogram image\n",
    "n_bands = 150\n",
    "# specify frames that you want to use. This is also the \"width\" of the spectrogram image\n",
    "n_frames = 150\n",
    "\n",
    "\n",
    "# sample rate of the target files\n",
    "sample_rate = 22050\n",
    "# update this part to produce different images\n",
    "save_dir = \"/mnt/us8k-\" + str(n_bands) + \"bands-\" + str(n_frames) + \"frames-3channel\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the length of the sliding window used to featurize the data into a mel spectrogram is empirical â€“ based on [Environmental sound classification with convolutional neural networks](http://karol.piczak.com/papers/Piczak2015-ESC-ConvNet.pdf) paper by Piczak, longer windows seems to perform better than shorter windows. In this blog, we will use a sliding window with a length of 2s with a 1 second overlapping; this will also determines the width of our spectrogram. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read wav helper method to force audio resampling\n",
    "# duration is set for a 4 second clip\n",
    "def read_audio(audio_path, target_fs=None, duration=4):\n",
    "    (audio, fs) = librosa.load(audio_path, sr=None, duration=duration)\n",
    "    # if this is not a mono sounds file\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    if target_fs is not None and fs != target_fs:\n",
    "        audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)\n",
    "        fs = target_fs\n",
    "    return audio, fs\n",
    "\n",
    "def pad_trunc_seq_rewrite(x, max_len):\n",
    "    \"\"\"Pad or truncate a sequence data to a fixed length.\n",
    "\n",
    "    Args:\n",
    "      x: ndarray, input sequence data.\n",
    "      max_len: integer, length of sequence to be padded or truncated.\n",
    "\n",
    "    Returns:\n",
    "      ndarray, Padded or truncated input sequence data.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.shape[1] < max_len:\n",
    "        pad_shape = (x.shape[0], max_len - x.shape[1])\n",
    "        pad = np.ones(pad_shape) * np.log(1e-8)\n",
    "        #x_new = np.concatenate((x, pad), axis=1)\n",
    "        x_new = np.hstack((x, pad))\n",
    "    # no pad necessary - truncate\n",
    "    else:\n",
    "        x_new = x[:, 0:max_len]\n",
    "    return x_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(parent_dir, sub_dirs, bands, frames, file_ext=\"*.wav\"):\n",
    "    # 4 second clip with 50% window overlap with small offset to guarantee frames\n",
    "    n_window = int(sample_rate * 4. / frames * 2) - 4 * 2\n",
    "    # 50% overlap\n",
    "    n_overlap = int(n_window / 2.)\n",
    "    # Mel filter bank\n",
    "    melW = librosa.filters.mel(sr=sample_rate, n_fft=n_window, n_mels=bands, fmin=0., fmax=8000.)\n",
    "    # Hamming window\n",
    "    ham_win = np.hamming(n_window)\n",
    "    log_specgrams_list = []\n",
    "    labels = []\n",
    "    for l, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            # print(\"processing\", fn)\n",
    "            sound_clip, fn_fs = read_audio(fn, target_fs=sample_rate)\n",
    "            assert (int(fn_fs) == sample_rate)\n",
    "\n",
    "            if sound_clip.shape[0] < n_window:\n",
    "                print(\"File %s is shorter than window size - DISCARDING - look into making the window larger.\" % fn)\n",
    "                continue\n",
    "\n",
    "            label = fn.split('fold')[1].split('-')[1]\n",
    "            # Skip corrupted wavs\n",
    "            if sound_clip.shape[0] == 0:\n",
    "                print(\"File %s is corrupted!\" % fn)\n",
    "                continue\n",
    "                # raise NameError(\"Check filename - it's an empty sound clip.\")\n",
    "\n",
    "            # Compute spectrogram                \n",
    "            [f, t, x] = signal.spectral.spectrogram(\n",
    "                x=sound_clip,\n",
    "                window=ham_win,\n",
    "                nperseg=n_window,\n",
    "                noverlap=n_overlap,\n",
    "                detrend=False,\n",
    "                return_onesided=True,\n",
    "                mode='magnitude')\n",
    "            x = np.dot(x.T, melW.T)\n",
    "            x = np.log(x + 1e-8)\n",
    "            x = x.astype(np.float32).T\n",
    "            x = pad_trunc_seq_rewrite(x, frames)\n",
    "\n",
    "            log_specgrams_list.append(x)\n",
    "            labels.append(label)\n",
    "\n",
    "    log_specgrams = np.asarray(log_specgrams_list).reshape(len(log_specgrams_list), bands, frames, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis=3)\n",
    "    features = np.concatenate((features, np.zeros(np.shape(log_specgrams))), axis=3)\n",
    "    for i in range(len(features)):\n",
    "        # first order difference, computed over 9-step window\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "        # for using 3 dimensional array to use ResNet and other frameworks\n",
    "        features[i, :, :, 2] = librosa.feature.delta(features[i, :, :, 1])\n",
    "\n",
    "    return np.array(features), np.array(labels, dtype=np.int)\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = num_total_classes\n",
    "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Extracted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below can convert the raw audio files into features using multi-processing to fully utilize the CPU. The processed data are stored as numpy arrays and will be loaded during training time.\n",
    "\n",
    "It takes around 10 mins to complete - the time will vary depending on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fold1\n",
      "Saving fold3\n",
      "Saving fold7\n",
      "Saving fold6\n",
      "Saving fold2\n",
      "Saving fold9\n",
      "Saving fold8\n",
      "Saving fold5\n",
      "Saving fold10\n",
      "Saving fold4\n",
      "File /mnt/UrbanSound8K/audio/fold1/87275-1-2-0.wav is shorter than window size - DISCARDING - look into making the window larger.\n",
      "File /mnt/UrbanSound8K/audio/fold1/87275-1-1-0.wav is shorter than window size - DISCARDING - look into making the window larger.\n",
      "Features of fold6  =  (823, 150, 150, 3)\n",
      "Labels of fold6  =  (823, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold6_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold6_y.npy\n",
      "Features of fold7  =  (838, 150, 150, 3)\n",
      "Labels of fold7  =  (838, 10)\n",
      "Features of fold3  =  (925, 150, 150, 3)\n",
      "Labels of fold3  =  (925, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold7_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold7_y.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold3_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold3_y.npy\n",
      "Features of fold8  =  (806, 150, 150, 3)\n",
      "Labels of fold8  =  (806, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold8_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold8_y.npy\n",
      "Features of fold10  =  (837, 150, 150, 3)\n",
      "Labels of fold10  =  (837, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold10_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold10_y.npy\n",
      "Features of fold9  =  (816, 150, 150, 3)\n",
      "Labels of fold9  =  (816, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold9_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold9_y.npy\n",
      "Features of fold5  =  (936, 150, 150, 3)\n",
      "Labels of fold5  =  (936, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold5_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold5_y.npy\n",
      "Features of fold2  =  (888, 150, 150, 3)\n",
      "Labels of fold2  =  (888, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold2_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold2_y.npy\n",
      "Features of fold1  =  (871, 150, 150, 3)\n",
      "Labels of fold1  =  (871, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold1_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold1_y.npy\n",
      "Features of fold4  =  (990, 150, 150, 3)\n",
      "Labels of fold4  =  (990, 10)\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold4_x.npy\n",
      "Saved /mnt/us8k-150bands-150frames-3channel/fold4_y.npy\n",
      "CPU times: user 823 ms, sys: 208 ms, total: 1.03 s\n",
      "Wall time: 20min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this to process the audio files into numpy arrays\n",
    "def save_folds(data_dir, k, bands, frames):\n",
    "    fold_name = 'fold' + str(k)\n",
    "    print(\"Saving \" + fold_name)\n",
    "\n",
    "    features, labels = extract_features(parent_dir, [fold_name], bands=bands, frames=frames)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    print(\"Features of\", fold_name, \" = \", features.shape)\n",
    "    print(\"Labels of\", fold_name, \" = \", labels.shape)\n",
    "\n",
    "    feature_file = os.path.join(data_dir, fold_name + '_x.npy')\n",
    "    labels_file = os.path.join(data_dir, fold_name + '_y.npy')\n",
    "    np.save(feature_file, features)\n",
    "    print(\"Saved \" + feature_file)\n",
    "    np.save(labels_file, labels)\n",
    "    print(\"Saved \" + labels_file)\n",
    "\n",
    "\n",
    "def assure_path_exists(path):\n",
    "    mydir = os.path.join(os.getcwd(), path)\n",
    "    if not os.path.exists(mydir):\n",
    "        os.makedirs(mydir)\n",
    "\n",
    "\n",
    "assure_path_exists(save_dir)\n",
    "Parallel(n_jobs=num_total_classes)(delayed(save_folds)(save_dir, k, bands=n_bands, frames=n_frames) for k in range(1, 11))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
