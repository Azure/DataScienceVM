{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Train and predict on urbansound8k dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this Jupyter Notebook, we will train and predict on UrbanSound8K dataset. There are a few published benchmarks, notebly mentioned in the papers below:\n",
    "\n",
    "- [Environmental sound classification with convolutional neural networks](http://karol.piczak.com/papers/Piczak2015-ESC-ConvNet.pdf) by Karol J Piczak.\n",
    "- [Deep convolutional neural networks and data augmentation for environmental sound classification](https://arxiv.org/abs/1608.04363) by Justin Salamon and Juan Pablo Bello\n",
    "- [Learning from Between-class Examples for Deep Sound Recognition](https://arxiv.org/abs/1711.10282) by Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada\n",
    "\n",
    "And the third one achieves the state-of-art performance, with an error rate around 21.7. In this notebook, we will demostrate how to use Azure DSVM to achive similar performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "keras.backend.clear_session()\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import Adamax\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "tf.set_random_seed(20171218)\n",
    "np.random.seed(20171218)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "frames = 150\n",
    "bands = 150\n",
    "feature_size = bands * frames\n",
    "data_dir = \"/home/maxkaz/Downloads/us8k-\" + str(bands) + \"bands-\" + str(frames) + \"frames-3channel\"\n",
    "num_channels = 3\n",
    "num_labels = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "If you're going to run this code with the full data set, this notebook assumes you've already parsed all the files and saved the numpy array to disk. We will load all the training examples (a set of 43722 examples, the first 8 folds), then use fold9 as the validation fold, and use fold 10 as the test fold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# this will aggregate all the training data \n",
    "def load_all_folds(test_fold):\n",
    "    assert (type(test_fold) == int)\n",
    "    assert (test_fold > 0 and test_fold < 11)\n",
    "    subsequent_fold = False\n",
    "\n",
    "    train_set_range = list(range(1, 11))\n",
    "    train_set_range.remove(test_fold)\n",
    "    valid_fold = train_set_range.pop()\n",
    "\n",
    "    for k in train_set_range:\n",
    "        fold_name = 'fold' + str(k)\n",
    "        feature_file = os.path.join(data_dir, fold_name + '_x.npy')\n",
    "        labels_file = os.path.join(data_dir, fold_name + '_y.npy')\n",
    "        loaded_features = np.load(feature_file)\n",
    "        # flip the spectrogram for each channel\n",
    "        loaded_features = np.transpose(loaded_features, (0, 2, 1, 3))\n",
    "        loaded_labels = np.load(labels_file)\n",
    "        print(\"Adding \", fold_name, \"New Features: \", loaded_features.shape)\n",
    "\n",
    "        if subsequent_fold:\n",
    "            train_x_loaded = np.concatenate((train_x_loaded, loaded_features))\n",
    "            train_y_loaded = np.concatenate((train_y_loaded, loaded_labels))\n",
    "        else:\n",
    "            train_x_loaded = loaded_features\n",
    "            train_y_loaded = loaded_labels\n",
    "            subsequent_fold = True\n",
    "\n",
    "    # use the penultimate fold for validation\n",
    "    valid_fold_name = 'fold' + str(valid_fold)\n",
    "    feature_file = os.path.join(data_dir, valid_fold_name + '_x.npy')\n",
    "    labels_file = os.path.join(data_dir, valid_fold_name + '_y.npy')\n",
    "    valid_x = np.load(feature_file)\n",
    "    # flip the spectrogram for each channel\n",
    "    valid_x = np.transpose(valid_x, (0, 2, 1, 3))\n",
    "    valid_y = np.load(labels_file)\n",
    "\n",
    "    # and use the last fold for testing\n",
    "    test_fold_name = 'fold' + str(test_fold)\n",
    "    feature_file = os.path.join(data_dir, test_fold_name + '_x.npy')\n",
    "    labels_file = os.path.join(data_dir, test_fold_name + '_y.npy')\n",
    "    test_x = np.load(feature_file)\n",
    "    test_x = np.transpose(test_x, (0, 2, 1, 3))\n",
    "    test_y = np.load(labels_file)\n",
    "    return train_x_loaded, train_y_loaded, valid_x, valid_y, test_x, test_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training a Convolutional Neural Network with Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "First, the imports we need and a few configuration variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This method defines some evaluation metrics that will be used to evaluate the performance of a trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_x, test_y):\n",
    "    y_prob = model.predict(test_x, verbose=0)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "    y_true = np.argmax(test_y, 1)\n",
    "\n",
    "    # evaluate the model\n",
    "    score, accuracy = model.evaluate(test_x, test_y, batch_size=32)\n",
    "    print(\"\\nAccuracy = {:.4f}\".format(accuracy))\n",
    "    print(\"\\nError Rate = {:.4f}\".format(1. - accuracy))\n",
    "\n",
    "    # the F-score gives a similiar value to the accuracy score, but useful for cross-checking\n",
    "    p, r, f, s = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    print(\"F-Score = {:.4f}\", f)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We use the same DNN architecture on featurized data as the winning solution to DCASE 2016 Track 4. DCASE is the audio challenge for sound domain and is held every year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # section 1\n",
    "\n",
    "    model.add(Convolution2D(filters=32, kernel_size=5,\n",
    "                            strides=2,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\",\n",
    "                            input_shape=(frames, bands, num_channels)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=32, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 2    \n",
    "    model.add(Convolution2D(filters=64, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=64, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 3\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Convolution2D(filters=128, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"same\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    # section 4\n",
    "    model.add(Convolution2D(filters=512, kernel_size=3,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Convolution2D(filters=512, kernel_size=1,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # section 5\n",
    "    model.add(Convolution2D(filters=10, kernel_size=1,\n",
    "                            strides=1,\n",
    "                            padding=\"valid\",\n",
    "                            kernel_regularizer=l2(0.0001),\n",
    "                            kernel_initializer=\"normal\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile and fit model, reduce epochs if you want a result faster\n",
    "    # the validation set is used to identify parameter settings (epoch) that achieves \n",
    "    # the highest classification accuracy\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# apply scaling factor to a dataset - train, validation or test\n",
    "def do_scale(x4d, verbose = True):\n",
    "    \"\"\"Do scale on the input sequence data.\n",
    "\n",
    "    Args:\n",
    "      x34d: ndarray, input sequence data, shape: (n_clips, n_time, n_freq, channel)      \n",
    "      verbose: boolean\n",
    "\n",
    "    Returns:\n",
    "      Scaled input sequence data.\n",
    "    \"\"\"\n",
    "    t1 = time.time()    \n",
    "    (n_clips, n_time, n_freq, n_channel) = x4d.shape\n",
    "    x4d_scaled = np.zeros(x4d.shape)\n",
    "    for channel in range(n_channel):\n",
    "        x2d = x4d[:,:,:,channel].reshape((n_clips * n_time, n_freq))\n",
    "        x2d_scaled = scaler_list[channel].transform(x2d)\n",
    "        x3d_scaled = x2d_scaled.reshape((n_clips, n_time, n_freq))\n",
    "        x4d_scaled[:,:,:,channel] = x3d_scaled\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(\"Scaling time: %s\" % (time.time() - t1,))\n",
    "\n",
    "    return x4d_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.903327703475952\n",
      "Scaling time: 1.1635143756866455\n",
      "Scaling time: 1.2766406536102295\n",
      "(7022, 150, 150, 3)\n",
      "Train on 7022 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "7022/7022 [==============================] - 11s 2ms/step - loss: 2.1900 - acc: 0.3606 - val_loss: 3.0165 - val_acc: 0.2174\n",
      "Epoch 2/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 1.8162 - acc: 0.5111 - val_loss: 2.2186 - val_acc: 0.3369\n",
      "Epoch 3/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 1.5461 - acc: 0.5830 - val_loss: 2.3692 - val_acc: 0.3118\n",
      "Epoch 4/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 1.3372 - acc: 0.6535 - val_loss: 1.8202 - val_acc: 0.4659\n",
      "Epoch 5/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 1.1907 - acc: 0.6987 - val_loss: 1.7862 - val_acc: 0.4671\n",
      "Epoch 6/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 1.1099 - acc: 0.7177 - val_loss: 2.1757 - val_acc: 0.3286\n",
      "Epoch 7/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 1.0451 - acc: 0.7344 - val_loss: 1.7315 - val_acc: 0.5424\n",
      "Epoch 8/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.9413 - acc: 0.7680 - val_loss: 1.8593 - val_acc: 0.4863\n",
      "Epoch 9/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.8885 - acc: 0.7760 - val_loss: 1.7895 - val_acc: 0.5293\n",
      "Epoch 10/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.8186 - acc: 0.8038 - val_loss: 1.6836 - val_acc: 0.5460\n",
      "Epoch 11/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.7861 - acc: 0.8099 - val_loss: 1.6090 - val_acc: 0.5484\n",
      "Epoch 12/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.7331 - acc: 0.8325 - val_loss: 1.3228 - val_acc: 0.6225\n",
      "Epoch 13/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.7008 - acc: 0.8391 - val_loss: 1.4885 - val_acc: 0.5914\n",
      "Epoch 14/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.6610 - acc: 0.8500 - val_loss: 1.5345 - val_acc: 0.5376\n",
      "Epoch 15/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.6363 - acc: 0.8587 - val_loss: 1.1882 - val_acc: 0.6834\n",
      "Epoch 16/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.5870 - acc: 0.8790 - val_loss: 1.2836 - val_acc: 0.6535\n",
      "Epoch 17/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.5245 - acc: 0.8986 - val_loss: 1.2850 - val_acc: 0.6356\n",
      "Epoch 18/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.5043 - acc: 0.8996 - val_loss: 1.3577 - val_acc: 0.6141\n",
      "Epoch 19/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.5038 - acc: 0.8993 - val_loss: 1.6351 - val_acc: 0.5711\n",
      "Epoch 20/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.4692 - acc: 0.9127 - val_loss: 1.2292 - val_acc: 0.6762\n",
      "Epoch 21/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.4361 - acc: 0.9220 - val_loss: 1.0658 - val_acc: 0.7097\n",
      "Epoch 22/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.4483 - acc: 0.9157 - val_loss: 1.3847 - val_acc: 0.6786\n",
      "Epoch 23/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.4430 - acc: 0.9185 - val_loss: 1.9269 - val_acc: 0.5556\n",
      "Epoch 24/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3934 - acc: 0.9348 - val_loss: 1.6327 - val_acc: 0.5544\n",
      "Epoch 25/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3761 - acc: 0.9355 - val_loss: 1.1656 - val_acc: 0.6691\n",
      "Epoch 26/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3503 - acc: 0.9426 - val_loss: 1.0162 - val_acc: 0.7575\n",
      "Epoch 27/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3465 - acc: 0.9497 - val_loss: 1.2024 - val_acc: 0.6762\n",
      "Epoch 28/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3324 - acc: 0.9547 - val_loss: 1.1046 - val_acc: 0.7168\n",
      "Epoch 29/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3183 - acc: 0.9546 - val_loss: 1.0357 - val_acc: 0.7228\n",
      "Epoch 30/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3031 - acc: 0.9567 - val_loss: 1.3658 - val_acc: 0.6464\n",
      "Epoch 31/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.3096 - acc: 0.9536 - val_loss: 1.3612 - val_acc: 0.6834\n",
      "Epoch 32/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2983 - acc: 0.9570 - val_loss: 0.8853 - val_acc: 0.7945\n",
      "Epoch 33/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2829 - acc: 0.9623 - val_loss: 1.0228 - val_acc: 0.7515\n",
      "Epoch 34/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2736 - acc: 0.9662 - val_loss: 1.0247 - val_acc: 0.7419\n",
      "Epoch 35/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2842 - acc: 0.9600 - val_loss: 1.2901 - val_acc: 0.6260\n",
      "Epoch 36/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2804 - acc: 0.9635 - val_loss: 1.0458 - val_acc: 0.7121\n",
      "Epoch 37/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2836 - acc: 0.9611 - val_loss: 1.1538 - val_acc: 0.7192\n",
      "Epoch 38/100\n",
      "6912/7022 [============================>.] - ETA: 0s - loss: 0.2656 - acc: 0.9688\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2659 - acc: 0.9688 - val_loss: 1.4981 - val_acc: 0.6057\n",
      "Epoch 39/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.2317 - acc: 0.9779 - val_loss: 0.9849 - val_acc: 0.7503\n",
      "Epoch 40/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1952 - acc: 0.9892 - val_loss: 0.8108 - val_acc: 0.8100\n",
      "Epoch 41/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1860 - acc: 0.9902 - val_loss: 0.8250 - val_acc: 0.8053\n",
      "Epoch 42/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1793 - acc: 0.9930 - val_loss: 0.7942 - val_acc: 0.8088\n",
      "Epoch 43/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1765 - acc: 0.9923 - val_loss: 0.7763 - val_acc: 0.8220\n",
      "Epoch 44/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1697 - acc: 0.9937 - val_loss: 0.7620 - val_acc: 0.8303\n",
      "Epoch 45/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1685 - acc: 0.9937 - val_loss: 0.7765 - val_acc: 0.8196\n",
      "Epoch 46/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1668 - acc: 0.9927 - val_loss: 0.7892 - val_acc: 0.8208\n",
      "Epoch 47/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1621 - acc: 0.9956 - val_loss: 0.7636 - val_acc: 0.8256\n",
      "Epoch 48/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1594 - acc: 0.9947 - val_loss: 0.7816 - val_acc: 0.8196\n",
      "Epoch 49/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1551 - acc: 0.9946 - val_loss: 0.7538 - val_acc: 0.8244\n",
      "Epoch 50/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1511 - acc: 0.9960 - val_loss: 0.7237 - val_acc: 0.8280\n",
      "Epoch 51/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1488 - acc: 0.9952 - val_loss: 0.7393 - val_acc: 0.8292\n",
      "Epoch 52/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1464 - acc: 0.9966 - val_loss: 0.7408 - val_acc: 0.8339\n",
      "Epoch 53/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1444 - acc: 0.9963 - val_loss: 0.7431 - val_acc: 0.8232\n",
      "Epoch 54/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1429 - acc: 0.9950 - val_loss: 0.7796 - val_acc: 0.8100\n",
      "Epoch 55/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1416 - acc: 0.9952 - val_loss: 0.7455 - val_acc: 0.8327\n",
      "Epoch 56/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1380 - acc: 0.9972 - val_loss: 0.6983 - val_acc: 0.8351\n",
      "Epoch 57/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1367 - acc: 0.9966 - val_loss: 0.7841 - val_acc: 0.8184\n",
      "Epoch 58/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1348 - acc: 0.9956 - val_loss: 0.7414 - val_acc: 0.8303\n",
      "Epoch 59/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1312 - acc: 0.9967 - val_loss: 0.7230 - val_acc: 0.8363\n",
      "Epoch 60/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1289 - acc: 0.9964 - val_loss: 0.7138 - val_acc: 0.8303\n",
      "Epoch 61/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1278 - acc: 0.9962 - val_loss: 0.7458 - val_acc: 0.8220\n",
      "Epoch 62/100\n",
      "6912/7022 [============================>.] - ETA: 0s - loss: 0.1249 - acc: 0.9970\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1252 - acc: 0.9969 - val_loss: 0.7495 - val_acc: 0.8112\n",
      "Epoch 63/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1228 - acc: 0.9970 - val_loss: 0.7237 - val_acc: 0.8244\n",
      "Epoch 64/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1216 - acc: 0.9972 - val_loss: 0.7276 - val_acc: 0.8327\n",
      "Epoch 65/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1209 - acc: 0.9979 - val_loss: 0.7249 - val_acc: 0.8315\n",
      "Epoch 66/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1201 - acc: 0.9973 - val_loss: 0.7255 - val_acc: 0.8339\n",
      "Epoch 67/100\n",
      "6912/7022 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9967\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1220 - acc: 0.9967 - val_loss: 0.7172 - val_acc: 0.8375\n",
      "Epoch 68/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1215 - acc: 0.9973 - val_loss: 0.7160 - val_acc: 0.8399\n",
      "Epoch 69/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1187 - acc: 0.9979 - val_loss: 0.7147 - val_acc: 0.8411\n",
      "Epoch 70/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1188 - acc: 0.9980 - val_loss: 0.7136 - val_acc: 0.8411\n",
      "Epoch 71/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1200 - acc: 0.9976 - val_loss: 0.7133 - val_acc: 0.8399\n",
      "Epoch 72/100\n",
      "6912/7022 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9987\n",
      "Epoch 00072: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1189 - acc: 0.9986 - val_loss: 0.7136 - val_acc: 0.8399\n",
      "Epoch 73/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1195 - acc: 0.9980 - val_loss: 0.7130 - val_acc: 0.8411\n",
      "Epoch 74/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1188 - acc: 0.9983 - val_loss: 0.7121 - val_acc: 0.8399\n",
      "Epoch 75/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1198 - acc: 0.9977 - val_loss: 0.7114 - val_acc: 0.8399\n",
      "Epoch 76/100\n",
      "7022/7022 [==============================] - 8s 1ms/step - loss: 0.1189 - acc: 0.9983 - val_loss: 0.7118 - val_acc: 0.8399\n",
      "Epoch 00076: early stopping\n",
      "871/871 [==============================] - 0s 449us/step\n",
      "\n",
      "Accuracy = 0.7658\n",
      "\n",
      "Error Rate = 0.2342\n",
      "F-Score = {:.4f} 0.765786452354\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.588601112365723\n",
      "Scaling time: 1.165006399154663\n",
      "Scaling time: 1.2567939758300781\n",
      "(7005, 150, 150, 3)\n",
      "Train on 7005 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "7005/7005 [==============================] - 9s 1ms/step - loss: 2.2366 - acc: 0.3205 - val_loss: 4.3705 - val_acc: 0.2067\n",
      "Epoch 2/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 1.7914 - acc: 0.5082 - val_loss: 1.9868 - val_acc: 0.4432\n",
      "Epoch 3/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 1.4935 - acc: 0.6161 - val_loss: 1.9790 - val_acc: 0.4146\n",
      "Epoch 4/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 1.3175 - acc: 0.6642 - val_loss: 2.3453 - val_acc: 0.3656\n",
      "Epoch 5/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 1.1609 - acc: 0.7162 - val_loss: 1.9120 - val_acc: 0.4205\n",
      "Epoch 6/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 1.0366 - acc: 0.7498 - val_loss: 1.6437 - val_acc: 0.4958\n",
      "Epoch 7/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.9986 - acc: 0.7527 - val_loss: 1.7745 - val_acc: 0.4588\n",
      "Epoch 8/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.9278 - acc: 0.7722 - val_loss: 1.3734 - val_acc: 0.6583\n",
      "Epoch 9/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.8533 - acc: 0.7981 - val_loss: 1.2874 - val_acc: 0.6643\n",
      "Epoch 10/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.8119 - acc: 0.8077 - val_loss: 2.0431 - val_acc: 0.4385\n",
      "Epoch 11/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.7446 - acc: 0.8320 - val_loss: 1.3378 - val_acc: 0.6667\n",
      "Epoch 12/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.7094 - acc: 0.8378 - val_loss: 1.3339 - val_acc: 0.6284\n",
      "Epoch 13/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.6673 - acc: 0.8488 - val_loss: 2.0272 - val_acc: 0.4182\n",
      "Epoch 14/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.6358 - acc: 0.8575 - val_loss: 1.2900 - val_acc: 0.6428\n",
      "Epoch 15/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.5889 - acc: 0.8715 - val_loss: 1.2143 - val_acc: 0.6726\n",
      "Epoch 16/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.5616 - acc: 0.8795 - val_loss: 1.3915 - val_acc: 0.6177\n",
      "Epoch 17/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.5119 - acc: 0.8989 - val_loss: 1.4107 - val_acc: 0.6165\n",
      "Epoch 18/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.4891 - acc: 0.9038 - val_loss: 1.7084 - val_acc: 0.4910\n",
      "Epoch 19/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.4659 - acc: 0.9108 - val_loss: 1.6335 - val_acc: 0.5747\n",
      "Epoch 20/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.4551 - acc: 0.9128 - val_loss: 1.9899 - val_acc: 0.4373\n",
      "Epoch 21/100\n",
      "6912/7005 [============================>.] - ETA: 0s - loss: 0.4273 - acc: 0.9190\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.4277 - acc: 0.9188 - val_loss: 1.3287 - val_acc: 0.6535\n",
      "Epoch 22/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.3572 - acc: 0.9458 - val_loss: 0.8997 - val_acc: 0.7706\n",
      "Epoch 23/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.3141 - acc: 0.9605 - val_loss: 0.8251 - val_acc: 0.8005\n",
      "Epoch 24/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2998 - acc: 0.9685 - val_loss: 0.7973 - val_acc: 0.8148\n",
      "Epoch 25/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2891 - acc: 0.9643 - val_loss: 0.7579 - val_acc: 0.8220\n",
      "Epoch 26/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2847 - acc: 0.9687 - val_loss: 0.7702 - val_acc: 0.8184\n",
      "Epoch 27/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2760 - acc: 0.9696 - val_loss: 0.7678 - val_acc: 0.8292\n",
      "Epoch 28/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2677 - acc: 0.9714 - val_loss: 0.7450 - val_acc: 0.8280\n",
      "Epoch 29/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2575 - acc: 0.9759 - val_loss: 0.7996 - val_acc: 0.8088\n",
      "Epoch 30/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2541 - acc: 0.9773 - val_loss: 0.7528 - val_acc: 0.8280\n",
      "Epoch 31/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2482 - acc: 0.9762 - val_loss: 0.7390 - val_acc: 0.8327\n",
      "Epoch 32/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2461 - acc: 0.9746 - val_loss: 0.7663 - val_acc: 0.8232\n",
      "Epoch 33/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2377 - acc: 0.9773 - val_loss: 0.7117 - val_acc: 0.8387\n",
      "Epoch 34/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2282 - acc: 0.9793 - val_loss: 0.7172 - val_acc: 0.8447\n",
      "Epoch 35/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2254 - acc: 0.9819 - val_loss: 0.7245 - val_acc: 0.8411\n",
      "Epoch 36/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2181 - acc: 0.9819 - val_loss: 0.7173 - val_acc: 0.8292\n",
      "Epoch 37/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2191 - acc: 0.9793 - val_loss: 0.7415 - val_acc: 0.8184\n",
      "Epoch 38/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2098 - acc: 0.9826 - val_loss: 0.8076 - val_acc: 0.7945\n",
      "Epoch 39/100\n",
      "6912/7005 [============================>.] - ETA: 0s - loss: 0.2044 - acc: 0.9861\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.2045 - acc: 0.9860 - val_loss: 0.7550 - val_acc: 0.8196\n",
      "Epoch 40/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1975 - acc: 0.9886 - val_loss: 0.7197 - val_acc: 0.8387\n",
      "Epoch 41/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1937 - acc: 0.9883 - val_loss: 0.7042 - val_acc: 0.8375\n",
      "Epoch 42/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1907 - acc: 0.9883 - val_loss: 0.6967 - val_acc: 0.8447\n",
      "Epoch 43/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1894 - acc: 0.9906 - val_loss: 0.7079 - val_acc: 0.8351\n",
      "Epoch 44/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1937 - acc: 0.9897 - val_loss: 0.7183 - val_acc: 0.8339\n",
      "Epoch 45/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1894 - acc: 0.9890 - val_loss: 0.7084 - val_acc: 0.8387\n",
      "Epoch 46/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1865 - acc: 0.9906 - val_loss: 0.7077 - val_acc: 0.8423\n",
      "Epoch 47/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1880 - acc: 0.9892 - val_loss: 0.6950 - val_acc: 0.8447\n",
      "Epoch 48/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1886 - acc: 0.9894 - val_loss: 0.7049 - val_acc: 0.8375\n",
      "Epoch 49/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1867 - acc: 0.9894 - val_loss: 0.7119 - val_acc: 0.8387\n",
      "Epoch 50/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1860 - acc: 0.9906 - val_loss: 0.7132 - val_acc: 0.8339\n",
      "Epoch 51/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1877 - acc: 0.9893 - val_loss: 0.7170 - val_acc: 0.8351\n",
      "Epoch 52/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1854 - acc: 0.9906 - val_loss: 0.7115 - val_acc: 0.8387\n",
      "Epoch 53/100\n",
      "6912/7005 [============================>.] - ETA: 0s - loss: 0.1862 - acc: 0.9889\n",
      "Epoch 00053: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1858 - acc: 0.9890 - val_loss: 0.7322 - val_acc: 0.8256\n",
      "Epoch 54/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1850 - acc: 0.9892 - val_loss: 0.7266 - val_acc: 0.8268\n",
      "Epoch 55/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1853 - acc: 0.9896 - val_loss: 0.7225 - val_acc: 0.8303\n",
      "Epoch 56/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1822 - acc: 0.9899 - val_loss: 0.7191 - val_acc: 0.8303\n",
      "Epoch 57/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1857 - acc: 0.9899 - val_loss: 0.7167 - val_acc: 0.8315\n",
      "Epoch 58/100\n",
      "6912/7005 [============================>.] - ETA: 0s - loss: 0.1812 - acc: 0.9913\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1817 - acc: 0.9913 - val_loss: 0.7173 - val_acc: 0.8303\n",
      "Epoch 59/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1850 - acc: 0.9901 - val_loss: 0.7166 - val_acc: 0.8303\n",
      "Epoch 60/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1885 - acc: 0.9890 - val_loss: 0.7162 - val_acc: 0.8315\n",
      "Epoch 61/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1800 - acc: 0.9909 - val_loss: 0.7156 - val_acc: 0.8327\n",
      "Epoch 62/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1829 - acc: 0.9893 - val_loss: 0.7159 - val_acc: 0.8327\n",
      "Epoch 63/100\n",
      "6912/7005 [============================>.] - ETA: 0s - loss: 0.1843 - acc: 0.9903\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1847 - acc: 0.9901 - val_loss: 0.7151 - val_acc: 0.8327\n",
      "Epoch 64/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1872 - acc: 0.9890 - val_loss: 0.7157 - val_acc: 0.8327\n",
      "Epoch 65/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1829 - acc: 0.9904 - val_loss: 0.7156 - val_acc: 0.8327\n",
      "Epoch 66/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1864 - acc: 0.9890 - val_loss: 0.7151 - val_acc: 0.8327\n",
      "Epoch 67/100\n",
      "7005/7005 [==============================] - 8s 1ms/step - loss: 0.1874 - acc: 0.9896 - val_loss: 0.7156 - val_acc: 0.8327\n",
      "Epoch 00067: early stopping\n",
      "888/888 [==============================] - 0s 445us/step\n",
      "\n",
      "Accuracy = 0.7883\n",
      "\n",
      "Error Rate = 0.2117\n",
      "F-Score = {:.4f} 0.788288288288\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.770503282546997\n",
      "Scaling time: 1.17767333984375\n",
      "Scaling time: 1.310631275177002\n",
      "(6968, 150, 150, 3)\n",
      "Train on 6968 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "6968/6968 [==============================] - 9s 1ms/step - loss: 2.1845 - acc: 0.3449 - val_loss: 2.9060 - val_acc: 0.2246\n",
      "Epoch 2/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 1.7711 - acc: 0.5060 - val_loss: 3.9027 - val_acc: 0.1386\n",
      "Epoch 3/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 1.5307 - acc: 0.5755 - val_loss: 2.1054 - val_acc: 0.3501\n",
      "Epoch 4/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 1.3440 - acc: 0.6316 - val_loss: 2.5005 - val_acc: 0.3082\n",
      "Epoch 5/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 1.2088 - acc: 0.6933 - val_loss: 2.3870 - val_acc: 0.3035\n",
      "Epoch 6/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 1.0995 - acc: 0.7263 - val_loss: 2.0058 - val_acc: 0.3823\n",
      "Epoch 7/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 1.0301 - acc: 0.7435 - val_loss: 2.0070 - val_acc: 0.3536\n",
      "Epoch 8/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.9616 - acc: 0.7678 - val_loss: 1.3206 - val_acc: 0.6284\n",
      "Epoch 9/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.8865 - acc: 0.7817 - val_loss: 1.3191 - val_acc: 0.5926\n",
      "Epoch 10/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.8572 - acc: 0.7989 - val_loss: 1.5300 - val_acc: 0.6105\n",
      "Epoch 11/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.8277 - acc: 0.7992 - val_loss: 1.2506 - val_acc: 0.6332\n",
      "Epoch 12/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.7591 - acc: 0.8263 - val_loss: 1.5185 - val_acc: 0.5341\n",
      "Epoch 13/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.7291 - acc: 0.8331 - val_loss: 1.7674 - val_acc: 0.4839\n",
      "Epoch 14/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.7110 - acc: 0.8370 - val_loss: 1.2674 - val_acc: 0.6703\n",
      "Epoch 15/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.6344 - acc: 0.8605 - val_loss: 2.0368 - val_acc: 0.5496\n",
      "Epoch 16/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.5994 - acc: 0.8662 - val_loss: 1.9577 - val_acc: 0.4683\n",
      "Epoch 17/100\n",
      "6912/6968 [============================>.] - ETA: 0s - loss: 0.5636 - acc: 0.8809\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.5640 - acc: 0.8807 - val_loss: 1.6818 - val_acc: 0.5484\n",
      "Epoch 18/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.4807 - acc: 0.9109 - val_loss: 1.1242 - val_acc: 0.7073\n",
      "Epoch 19/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.4370 - acc: 0.9261 - val_loss: 1.0728 - val_acc: 0.7157\n",
      "Epoch 20/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.4206 - acc: 0.9294 - val_loss: 0.9241 - val_acc: 0.7551\n",
      "Epoch 21/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.4034 - acc: 0.9353 - val_loss: 0.8603 - val_acc: 0.7778\n",
      "Epoch 22/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3955 - acc: 0.9406 - val_loss: 0.8449 - val_acc: 0.7802\n",
      "Epoch 23/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3836 - acc: 0.9429 - val_loss: 0.8292 - val_acc: 0.7861\n",
      "Epoch 24/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3783 - acc: 0.9414 - val_loss: 0.8079 - val_acc: 0.7826\n",
      "Epoch 25/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3726 - acc: 0.9435 - val_loss: 0.8016 - val_acc: 0.7945\n",
      "Epoch 26/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3648 - acc: 0.9463 - val_loss: 0.8018 - val_acc: 0.7885\n",
      "Epoch 27/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3520 - acc: 0.9496 - val_loss: 0.8023 - val_acc: 0.7897\n",
      "Epoch 28/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3419 - acc: 0.9503 - val_loss: 0.8082 - val_acc: 0.7694\n",
      "Epoch 29/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3420 - acc: 0.9495 - val_loss: 0.7857 - val_acc: 0.7873\n",
      "Epoch 30/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3250 - acc: 0.9577 - val_loss: 0.7558 - val_acc: 0.7921\n",
      "Epoch 31/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3194 - acc: 0.9572 - val_loss: 0.8307 - val_acc: 0.7658\n",
      "Epoch 32/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3162 - acc: 0.9552 - val_loss: 0.7705 - val_acc: 0.7838\n",
      "Epoch 33/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3118 - acc: 0.9590 - val_loss: 0.7480 - val_acc: 0.7981\n",
      "Epoch 34/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.3020 - acc: 0.9621 - val_loss: 0.7579 - val_acc: 0.7861\n",
      "Epoch 35/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2989 - acc: 0.9611 - val_loss: 0.7693 - val_acc: 0.7969\n",
      "Epoch 36/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2942 - acc: 0.9607 - val_loss: 0.7456 - val_acc: 0.8005\n",
      "Epoch 37/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2814 - acc: 0.9658 - val_loss: 0.7514 - val_acc: 0.7921\n",
      "Epoch 38/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2848 - acc: 0.9633 - val_loss: 0.7445 - val_acc: 0.7921\n",
      "Epoch 39/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2820 - acc: 0.9668 - val_loss: 0.7608 - val_acc: 0.7933\n",
      "Epoch 40/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2713 - acc: 0.9670 - val_loss: 0.7304 - val_acc: 0.7921\n",
      "Epoch 41/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2680 - acc: 0.9697 - val_loss: 0.7625 - val_acc: 0.7861\n",
      "Epoch 42/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2600 - acc: 0.9727 - val_loss: 0.7189 - val_acc: 0.8017\n",
      "Epoch 43/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2541 - acc: 0.9724 - val_loss: 0.7184 - val_acc: 0.7981\n",
      "Epoch 44/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2551 - acc: 0.9727 - val_loss: 0.7208 - val_acc: 0.7897\n",
      "Epoch 45/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2461 - acc: 0.9736 - val_loss: 0.7003 - val_acc: 0.8065\n",
      "Epoch 46/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2445 - acc: 0.9735 - val_loss: 0.6866 - val_acc: 0.8196\n",
      "Epoch 47/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2416 - acc: 0.9742 - val_loss: 0.7107 - val_acc: 0.8053\n",
      "Epoch 48/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2373 - acc: 0.9750 - val_loss: 0.6956 - val_acc: 0.8100\n",
      "Epoch 49/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2293 - acc: 0.9778 - val_loss: 0.6526 - val_acc: 0.8184\n",
      "Epoch 50/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2375 - acc: 0.9727 - val_loss: 0.7105 - val_acc: 0.7993\n",
      "Epoch 51/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2280 - acc: 0.9783 - val_loss: 0.6697 - val_acc: 0.8136\n",
      "Epoch 52/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2226 - acc: 0.9776 - val_loss: 0.7310 - val_acc: 0.7885\n",
      "Epoch 53/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2184 - acc: 0.9799 - val_loss: 0.7691 - val_acc: 0.7826\n",
      "Epoch 54/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2131 - acc: 0.9815 - val_loss: 0.6510 - val_acc: 0.8244\n",
      "Epoch 55/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2131 - acc: 0.9790 - val_loss: 0.6730 - val_acc: 0.8184\n",
      "Epoch 56/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2084 - acc: 0.9808 - val_loss: 0.6975 - val_acc: 0.8029\n",
      "Epoch 57/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2059 - acc: 0.9812 - val_loss: 0.6985 - val_acc: 0.8041\n",
      "Epoch 58/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2087 - acc: 0.9812 - val_loss: 0.6817 - val_acc: 0.8065\n",
      "Epoch 59/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.2019 - acc: 0.9832 - val_loss: 0.6752 - val_acc: 0.8076\n",
      "Epoch 60/100\n",
      "6912/6968 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.9857\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1991 - acc: 0.9854 - val_loss: 0.7431 - val_acc: 0.8005\n",
      "Epoch 61/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1890 - acc: 0.9858 - val_loss: 0.6606 - val_acc: 0.8196\n",
      "Epoch 62/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1851 - acc: 0.9884 - val_loss: 0.6501 - val_acc: 0.8268\n",
      "Epoch 63/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1833 - acc: 0.9892 - val_loss: 0.6386 - val_acc: 0.8315\n",
      "Epoch 64/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1810 - acc: 0.9892 - val_loss: 0.6373 - val_acc: 0.8268\n",
      "Epoch 65/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1818 - acc: 0.9887 - val_loss: 0.6372 - val_acc: 0.8292\n",
      "Epoch 66/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1812 - acc: 0.9889 - val_loss: 0.6444 - val_acc: 0.8244\n",
      "Epoch 67/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1798 - acc: 0.9887 - val_loss: 0.6469 - val_acc: 0.8244\n",
      "Epoch 68/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1775 - acc: 0.9908 - val_loss: 0.6493 - val_acc: 0.8208\n",
      "Epoch 69/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1798 - acc: 0.9888 - val_loss: 0.6508 - val_acc: 0.8268\n",
      "Epoch 70/100\n",
      "6912/6968 [============================>.] - ETA: 0s - loss: 0.1799 - acc: 0.9905\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1802 - acc: 0.9904 - val_loss: 0.6562 - val_acc: 0.8196\n",
      "Epoch 71/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1767 - acc: 0.9904 - val_loss: 0.6560 - val_acc: 0.8208\n",
      "Epoch 72/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1790 - acc: 0.9898 - val_loss: 0.6552 - val_acc: 0.8244\n",
      "Epoch 73/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1809 - acc: 0.9891 - val_loss: 0.6562 - val_acc: 0.8232\n",
      "Epoch 74/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1778 - acc: 0.9897 - val_loss: 0.6554 - val_acc: 0.8244\n",
      "Epoch 75/100\n",
      "6912/6968 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9912\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1758 - acc: 0.9910 - val_loss: 0.6546 - val_acc: 0.8220\n",
      "Epoch 76/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1828 - acc: 0.9871 - val_loss: 0.6557 - val_acc: 0.8220\n",
      "Epoch 77/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1785 - acc: 0.9888 - val_loss: 0.6543 - val_acc: 0.8232\n",
      "Epoch 78/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1768 - acc: 0.9908 - val_loss: 0.6559 - val_acc: 0.8220\n",
      "Epoch 79/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1762 - acc: 0.9904 - val_loss: 0.6564 - val_acc: 0.8220\n",
      "Epoch 80/100\n",
      "6912/6968 [============================>.] - ETA: 0s - loss: 0.1766 - acc: 0.9918\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1770 - acc: 0.9915 - val_loss: 0.6573 - val_acc: 0.8208\n",
      "Epoch 81/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1805 - acc: 0.9887 - val_loss: 0.6566 - val_acc: 0.8208\n",
      "Epoch 82/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1792 - acc: 0.9891 - val_loss: 0.6561 - val_acc: 0.8208\n",
      "Epoch 83/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1791 - acc: 0.9897 - val_loss: 0.6568 - val_acc: 0.8208\n",
      "Epoch 84/100\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1764 - acc: 0.9898 - val_loss: 0.6576 - val_acc: 0.8208\n",
      "Epoch 85/100\n",
      "6912/6968 [============================>.] - ETA: 0s - loss: 0.1782 - acc: 0.9910\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "6968/6968 [==============================] - 8s 1ms/step - loss: 0.1782 - acc: 0.9911 - val_loss: 0.6576 - val_acc: 0.8208\n",
      "Epoch 00085: early stopping\n",
      "925/925 [==============================] - 0s 448us/step\n",
      "\n",
      "Accuracy = 0.6930\n",
      "\n",
      "Error Rate = 0.3070\n",
      "F-Score = {:.4f} 0.692972972973\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.539489269256592\n",
      "Scaling time: 1.2534101009368896\n",
      "Scaling time: 1.4369866847991943\n",
      "(6903, 150, 150, 3)\n",
      "Train on 6903 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "6903/6903 [==============================] - 10s 1ms/step - loss: 2.2012 - acc: 0.3559 - val_loss: 3.6722 - val_acc: 0.1075\n",
      "Epoch 2/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 1.7982 - acc: 0.5017 - val_loss: 2.6018 - val_acc: 0.2760\n",
      "Epoch 3/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 1.5380 - acc: 0.5861 - val_loss: 2.2809 - val_acc: 0.3668\n",
      "Epoch 4/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 1.3576 - acc: 0.6448 - val_loss: 2.3866 - val_acc: 0.3692\n",
      "Epoch 5/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 1.1993 - acc: 0.6951 - val_loss: 2.1286 - val_acc: 0.3489\n",
      "Epoch 6/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 1.0990 - acc: 0.7253 - val_loss: 2.0690 - val_acc: 0.4301\n",
      "Epoch 7/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.9898 - acc: 0.7601 - val_loss: 1.9710 - val_acc: 0.4648\n",
      "Epoch 8/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.9423 - acc: 0.7666 - val_loss: 1.7185 - val_acc: 0.5197\n",
      "Epoch 9/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.8450 - acc: 0.7917 - val_loss: 1.6780 - val_acc: 0.5066\n",
      "Epoch 10/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.8336 - acc: 0.7966 - val_loss: 1.4102 - val_acc: 0.6117\n",
      "Epoch 11/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.7499 - acc: 0.8266 - val_loss: 1.2400 - val_acc: 0.6272\n",
      "Epoch 12/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.7150 - acc: 0.8344 - val_loss: 1.5643 - val_acc: 0.5699\n",
      "Epoch 13/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.6655 - acc: 0.8512 - val_loss: 1.3311 - val_acc: 0.6272\n",
      "Epoch 14/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.6070 - acc: 0.8705 - val_loss: 1.1686 - val_acc: 0.6703\n",
      "Epoch 15/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.5872 - acc: 0.8705 - val_loss: 1.2837 - val_acc: 0.6392\n",
      "Epoch 16/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.5677 - acc: 0.8803 - val_loss: 1.3246 - val_acc: 0.6607\n",
      "Epoch 17/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.5197 - acc: 0.8924 - val_loss: 1.4958 - val_acc: 0.5783\n",
      "Epoch 18/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.4876 - acc: 0.9063 - val_loss: 1.1351 - val_acc: 0.6822\n",
      "Epoch 19/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.4780 - acc: 0.9063 - val_loss: 1.5704 - val_acc: 0.6189\n",
      "Epoch 20/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.4737 - acc: 0.9102 - val_loss: 1.2342 - val_acc: 0.6631\n",
      "Epoch 21/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.4114 - acc: 0.9263 - val_loss: 1.6129 - val_acc: 0.5424\n",
      "Epoch 22/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.4114 - acc: 0.9244 - val_loss: 1.6171 - val_acc: 0.5544\n",
      "Epoch 23/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.4027 - acc: 0.9251 - val_loss: 1.2087 - val_acc: 0.6547\n",
      "Epoch 24/100\n",
      "6656/6903 [===========================>..] - ETA: 0s - loss: 0.3953 - acc: 0.9292\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.3949 - acc: 0.9290 - val_loss: 1.1849 - val_acc: 0.7109\n",
      "Epoch 25/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.3218 - acc: 0.9558 - val_loss: 0.8752 - val_acc: 0.7790\n",
      "Epoch 26/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2744 - acc: 0.9697 - val_loss: 0.8068 - val_acc: 0.7921\n",
      "Epoch 27/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2662 - acc: 0.9733 - val_loss: 0.7732 - val_acc: 0.8100\n",
      "Epoch 28/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2562 - acc: 0.9764 - val_loss: 0.7942 - val_acc: 0.7993\n",
      "Epoch 29/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2489 - acc: 0.9768 - val_loss: 0.7650 - val_acc: 0.7981\n",
      "Epoch 30/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2451 - acc: 0.9780 - val_loss: 0.7504 - val_acc: 0.7969\n",
      "Epoch 31/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2338 - acc: 0.9809 - val_loss: 0.7637 - val_acc: 0.8065\n",
      "Epoch 32/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2297 - acc: 0.9817 - val_loss: 0.7528 - val_acc: 0.8112\n",
      "Epoch 33/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2273 - acc: 0.9802 - val_loss: 0.7557 - val_acc: 0.8112\n",
      "Epoch 34/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2181 - acc: 0.9825 - val_loss: 0.7588 - val_acc: 0.7981\n",
      "Epoch 35/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2180 - acc: 0.9820 - val_loss: 0.7749 - val_acc: 0.7981\n",
      "Epoch 36/100\n",
      "6656/6903 [===========================>..] - ETA: 0s - loss: 0.2097 - acc: 0.9854\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2100 - acc: 0.9851 - val_loss: 0.7898 - val_acc: 0.7861\n",
      "Epoch 37/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2080 - acc: 0.9858 - val_loss: 0.7666 - val_acc: 0.7921\n",
      "Epoch 38/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2024 - acc: 0.9865 - val_loss: 0.7346 - val_acc: 0.7981\n",
      "Epoch 39/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2003 - acc: 0.9867 - val_loss: 0.7231 - val_acc: 0.8005\n",
      "Epoch 40/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1969 - acc: 0.9878 - val_loss: 0.7125 - val_acc: 0.8053\n",
      "Epoch 41/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.2006 - acc: 0.9870 - val_loss: 0.7112 - val_acc: 0.8076\n",
      "Epoch 42/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1995 - acc: 0.9865 - val_loss: 0.7078 - val_acc: 0.8065\n",
      "Epoch 43/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1997 - acc: 0.9868 - val_loss: 0.7183 - val_acc: 0.8065\n",
      "Epoch 44/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1991 - acc: 0.9871 - val_loss: 0.7095 - val_acc: 0.8076\n",
      "Epoch 45/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1973 - acc: 0.9878 - val_loss: 0.7038 - val_acc: 0.8100\n",
      "Epoch 46/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1972 - acc: 0.9873 - val_loss: 0.7043 - val_acc: 0.8112\n",
      "Epoch 47/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1978 - acc: 0.9852 - val_loss: 0.7042 - val_acc: 0.8100\n",
      "Epoch 48/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1951 - acc: 0.9887 - val_loss: 0.6988 - val_acc: 0.8076\n",
      "Epoch 49/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1929 - acc: 0.9883 - val_loss: 0.7043 - val_acc: 0.8100\n",
      "Epoch 50/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1954 - acc: 0.9880 - val_loss: 0.6978 - val_acc: 0.8160\n",
      "Epoch 51/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1941 - acc: 0.9880 - val_loss: 0.6983 - val_acc: 0.8136\n",
      "Epoch 52/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1927 - acc: 0.9880 - val_loss: 0.6883 - val_acc: 0.8136\n",
      "Epoch 53/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1913 - acc: 0.9878 - val_loss: 0.6890 - val_acc: 0.8136\n",
      "Epoch 54/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1903 - acc: 0.9887 - val_loss: 0.6909 - val_acc: 0.8124\n",
      "Epoch 55/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1903 - acc: 0.9894 - val_loss: 0.6972 - val_acc: 0.8124\n",
      "Epoch 56/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1932 - acc: 0.9884 - val_loss: 0.6995 - val_acc: 0.8112\n",
      "Epoch 57/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1898 - acc: 0.9909 - val_loss: 0.6945 - val_acc: 0.8172\n",
      "Epoch 58/100\n",
      "6656/6903 [===========================>..] - ETA: 0s - loss: 0.1910 - acc: 0.9892\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1909 - acc: 0.9891 - val_loss: 0.6896 - val_acc: 0.8148\n",
      "Epoch 59/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1884 - acc: 0.9881 - val_loss: 0.6890 - val_acc: 0.8172\n",
      "Epoch 60/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1875 - acc: 0.9883 - val_loss: 0.6893 - val_acc: 0.8172\n",
      "Epoch 61/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1888 - acc: 0.9891 - val_loss: 0.6908 - val_acc: 0.8160\n",
      "Epoch 62/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1896 - acc: 0.9884 - val_loss: 0.6907 - val_acc: 0.8172\n",
      "Epoch 63/100\n",
      "6656/6903 [===========================>..] - ETA: 0s - loss: 0.1874 - acc: 0.9895\n",
      "Epoch 00063: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1884 - acc: 0.9893 - val_loss: 0.6911 - val_acc: 0.8148\n",
      "Epoch 64/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1876 - acc: 0.9904 - val_loss: 0.6911 - val_acc: 0.8148\n",
      "Epoch 65/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1902 - acc: 0.9896 - val_loss: 0.6913 - val_acc: 0.8148\n",
      "Epoch 66/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1870 - acc: 0.9903 - val_loss: 0.6918 - val_acc: 0.8148\n",
      "Epoch 67/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1860 - acc: 0.9899 - val_loss: 0.6918 - val_acc: 0.8148\n",
      "Epoch 68/100\n",
      "6656/6903 [===========================>..] - ETA: 0s - loss: 0.1879 - acc: 0.9907\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1892 - acc: 0.9906 - val_loss: 0.6914 - val_acc: 0.8148\n",
      "Epoch 69/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1898 - acc: 0.9893 - val_loss: 0.6917 - val_acc: 0.8148\n",
      "Epoch 70/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1899 - acc: 0.9897 - val_loss: 0.6919 - val_acc: 0.8148\n",
      "Epoch 71/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1884 - acc: 0.9903 - val_loss: 0.6922 - val_acc: 0.8148\n",
      "Epoch 72/100\n",
      "6903/6903 [==============================] - 8s 1ms/step - loss: 0.1877 - acc: 0.9891 - val_loss: 0.6922 - val_acc: 0.8148\n",
      "Epoch 00072: early stopping\n",
      "990/990 [==============================] - 0s 440us/step\n",
      "\n",
      "Accuracy = 0.7980\n",
      "\n",
      "Error Rate = 0.2020\n",
      "F-Score = {:.4f} 0.79797979798\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.648764371871948\n",
      "Scaling time: 1.1729683876037598\n",
      "Scaling time: 1.2692992687225342\n",
      "(6957, 150, 150, 3)\n",
      "Train on 6957 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "6957/6957 [==============================] - 9s 1ms/step - loss: 2.2264 - acc: 0.3318 - val_loss: 2.8945 - val_acc: 0.2413\n",
      "Epoch 2/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 1.7723 - acc: 0.5149 - val_loss: 2.6343 - val_acc: 0.2306\n",
      "Epoch 3/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 1.5002 - acc: 0.6028 - val_loss: 3.3031 - val_acc: 0.2748\n",
      "Epoch 4/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 1.3343 - acc: 0.6428 - val_loss: 2.0207 - val_acc: 0.3536\n",
      "Epoch 5/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 1.2038 - acc: 0.6947 - val_loss: 1.9037 - val_acc: 0.4839\n",
      "Epoch 6/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 1.1238 - acc: 0.7107 - val_loss: 2.1190 - val_acc: 0.3728\n",
      "Epoch 7/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 1.0076 - acc: 0.7496 - val_loss: 2.3898 - val_acc: 0.3345\n",
      "Epoch 8/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.9709 - acc: 0.7548 - val_loss: 1.5351 - val_acc: 0.5161\n",
      "Epoch 9/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.9007 - acc: 0.7759 - val_loss: 2.0853 - val_acc: 0.4277\n",
      "Epoch 10/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.8642 - acc: 0.7867 - val_loss: 1.7808 - val_acc: 0.4827\n",
      "Epoch 11/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.8458 - acc: 0.7857 - val_loss: 1.9305 - val_acc: 0.5030\n",
      "Epoch 12/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.7751 - acc: 0.8127 - val_loss: 1.8738 - val_acc: 0.5352\n",
      "Epoch 13/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.7458 - acc: 0.8205 - val_loss: 1.6670 - val_acc: 0.5914\n",
      "Epoch 14/100\n",
      "6912/6957 [============================>.] - ETA: 0s - loss: 0.6921 - acc: 0.8430\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.6932 - acc: 0.8422 - val_loss: 1.6761 - val_acc: 0.5615\n",
      "Epoch 15/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.5997 - acc: 0.8793 - val_loss: 1.1920 - val_acc: 0.6941\n",
      "Epoch 16/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.5593 - acc: 0.8895 - val_loss: 1.1623 - val_acc: 0.7228\n",
      "Epoch 17/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.5344 - acc: 0.8988 - val_loss: 1.0374 - val_acc: 0.7384\n",
      "Epoch 18/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.5216 - acc: 0.9024 - val_loss: 0.9605 - val_acc: 0.7611\n",
      "Epoch 19/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.5112 - acc: 0.9057 - val_loss: 0.9696 - val_acc: 0.7443\n",
      "Epoch 20/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.5025 - acc: 0.9057 - val_loss: 0.9412 - val_acc: 0.7384\n",
      "Epoch 21/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4901 - acc: 0.9090 - val_loss: 0.9115 - val_acc: 0.7539\n",
      "Epoch 22/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4860 - acc: 0.9143 - val_loss: 0.8742 - val_acc: 0.7754\n",
      "Epoch 23/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4709 - acc: 0.9149 - val_loss: 0.9101 - val_acc: 0.7467\n",
      "Epoch 24/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4638 - acc: 0.9182 - val_loss: 0.9348 - val_acc: 0.7515\n",
      "Epoch 25/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4475 - acc: 0.9235 - val_loss: 0.8435 - val_acc: 0.7826\n",
      "Epoch 26/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4470 - acc: 0.9211 - val_loss: 0.8944 - val_acc: 0.7551\n",
      "Epoch 27/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4312 - acc: 0.9281 - val_loss: 0.8737 - val_acc: 0.7778\n",
      "Epoch 28/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4336 - acc: 0.9208 - val_loss: 0.9030 - val_acc: 0.7479\n",
      "Epoch 29/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4172 - acc: 0.9300 - val_loss: 0.8624 - val_acc: 0.7754\n",
      "Epoch 30/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4032 - acc: 0.9339 - val_loss: 0.8993 - val_acc: 0.7575\n",
      "Epoch 31/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4012 - acc: 0.9322 - val_loss: 0.8417 - val_acc: 0.7754\n",
      "Epoch 32/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.4010 - acc: 0.9333 - val_loss: 0.8009 - val_acc: 0.7814\n",
      "Epoch 33/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3885 - acc: 0.9385 - val_loss: 0.8531 - val_acc: 0.7622\n",
      "Epoch 34/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3820 - acc: 0.9406 - val_loss: 0.8387 - val_acc: 0.7646\n",
      "Epoch 35/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3761 - acc: 0.9414 - val_loss: 0.7613 - val_acc: 0.8017\n",
      "Epoch 36/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3651 - acc: 0.9412 - val_loss: 0.7971 - val_acc: 0.7826\n",
      "Epoch 37/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3650 - acc: 0.9426 - val_loss: 0.7810 - val_acc: 0.7694\n",
      "Epoch 38/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3552 - acc: 0.9478 - val_loss: 0.7434 - val_acc: 0.7981\n",
      "Epoch 39/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3418 - acc: 0.9516 - val_loss: 0.8184 - val_acc: 0.7754\n",
      "Epoch 40/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3386 - acc: 0.9503 - val_loss: 0.7727 - val_acc: 0.7802\n",
      "Epoch 41/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3317 - acc: 0.9508 - val_loss: 0.7387 - val_acc: 0.7897\n",
      "Epoch 42/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3260 - acc: 0.9577 - val_loss: 0.7859 - val_acc: 0.7742\n",
      "Epoch 43/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3211 - acc: 0.9544 - val_loss: 0.7863 - val_acc: 0.7945\n",
      "Epoch 44/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3213 - acc: 0.9552 - val_loss: 0.7214 - val_acc: 0.8100\n",
      "Epoch 45/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3098 - acc: 0.9575 - val_loss: 0.7259 - val_acc: 0.8100\n",
      "Epoch 46/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3063 - acc: 0.9590 - val_loss: 0.7343 - val_acc: 0.8065\n",
      "Epoch 47/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2992 - acc: 0.9598 - val_loss: 0.7919 - val_acc: 0.7790\n",
      "Epoch 48/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.3020 - acc: 0.9582 - val_loss: 0.7062 - val_acc: 0.8053\n",
      "Epoch 49/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2967 - acc: 0.9610 - val_loss: 0.7469 - val_acc: 0.7861\n",
      "Epoch 50/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2906 - acc: 0.9616 - val_loss: 0.7090 - val_acc: 0.7945\n",
      "Epoch 51/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2920 - acc: 0.9592 - val_loss: 0.7347 - val_acc: 0.8065\n",
      "Epoch 52/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2870 - acc: 0.9629 - val_loss: 0.7370 - val_acc: 0.7957\n",
      "Epoch 53/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2755 - acc: 0.9664 - val_loss: 0.7309 - val_acc: 0.7957\n",
      "Epoch 54/100\n",
      "6912/6957 [============================>.] - ETA: 0s - loss: 0.2723 - acc: 0.9690\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2726 - acc: 0.9690 - val_loss: 0.7377 - val_acc: 0.7826\n",
      "Epoch 55/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2594 - acc: 0.9688 - val_loss: 0.7137 - val_acc: 0.7981\n",
      "Epoch 56/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2568 - acc: 0.9718 - val_loss: 0.7123 - val_acc: 0.7981\n",
      "Epoch 57/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2572 - acc: 0.9737 - val_loss: 0.7036 - val_acc: 0.8065\n",
      "Epoch 58/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2564 - acc: 0.9723 - val_loss: 0.6984 - val_acc: 0.8053\n",
      "Epoch 59/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2532 - acc: 0.9756 - val_loss: 0.6989 - val_acc: 0.8041\n",
      "Epoch 60/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2516 - acc: 0.9743 - val_loss: 0.6991 - val_acc: 0.8041\n",
      "Epoch 61/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2552 - acc: 0.9721 - val_loss: 0.6863 - val_acc: 0.8041\n",
      "Epoch 62/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2542 - acc: 0.9733 - val_loss: 0.6754 - val_acc: 0.8076\n",
      "Epoch 63/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2506 - acc: 0.9723 - val_loss: 0.6831 - val_acc: 0.8053\n",
      "Epoch 64/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2525 - acc: 0.9736 - val_loss: 0.6835 - val_acc: 0.8100\n",
      "Epoch 65/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2471 - acc: 0.9748 - val_loss: 0.6860 - val_acc: 0.8053\n",
      "Epoch 66/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2481 - acc: 0.9760 - val_loss: 0.6972 - val_acc: 0.8029\n",
      "Epoch 67/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2481 - acc: 0.9760 - val_loss: 0.6869 - val_acc: 0.8041\n",
      "Epoch 68/100\n",
      "6912/6957 [============================>.] - ETA: 0s - loss: 0.2440 - acc: 0.9767\n",
      "Epoch 00068: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2442 - acc: 0.9766 - val_loss: 0.6874 - val_acc: 0.8029\n",
      "Epoch 69/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2487 - acc: 0.9734 - val_loss: 0.6860 - val_acc: 0.8041\n",
      "Epoch 70/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2419 - acc: 0.9767 - val_loss: 0.6865 - val_acc: 0.8017\n",
      "Epoch 71/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2438 - acc: 0.9770 - val_loss: 0.6869 - val_acc: 0.8005\n",
      "Epoch 72/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2468 - acc: 0.9754 - val_loss: 0.6901 - val_acc: 0.8017\n",
      "Epoch 73/100\n",
      "6912/6957 [============================>.] - ETA: 0s - loss: 0.2435 - acc: 0.9774\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2433 - acc: 0.9776 - val_loss: 0.6897 - val_acc: 0.8005\n",
      "Epoch 74/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2425 - acc: 0.9777 - val_loss: 0.6881 - val_acc: 0.8017\n",
      "Epoch 75/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2479 - acc: 0.9747 - val_loss: 0.6881 - val_acc: 0.8041\n",
      "Epoch 76/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2430 - acc: 0.9783 - val_loss: 0.6882 - val_acc: 0.8029\n",
      "Epoch 77/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2460 - acc: 0.9764 - val_loss: 0.6883 - val_acc: 0.8041\n",
      "Epoch 78/100\n",
      "6912/6957 [============================>.] - ETA: 0s - loss: 0.2457 - acc: 0.9757\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2463 - acc: 0.9754 - val_loss: 0.6867 - val_acc: 0.8041\n",
      "Epoch 79/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2488 - acc: 0.9746 - val_loss: 0.6872 - val_acc: 0.8041\n",
      "Epoch 80/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2504 - acc: 0.9740 - val_loss: 0.6860 - val_acc: 0.8041\n",
      "Epoch 81/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2464 - acc: 0.9753 - val_loss: 0.6858 - val_acc: 0.8053\n",
      "Epoch 82/100\n",
      "6957/6957 [==============================] - 8s 1ms/step - loss: 0.2465 - acc: 0.9766 - val_loss: 0.6859 - val_acc: 0.8053\n",
      "Epoch 00082: early stopping\n",
      "936/936 [==============================] - 0s 439us/step\n",
      "\n",
      "Accuracy = 0.8472\n",
      "\n",
      "Error Rate = 0.1528\n",
      "F-Score = {:.4f} 0.847222222222\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.92221188545227\n",
      "Scaling time: 1.1745007038116455\n",
      "Scaling time: 1.170137643814087\n",
      "(7070, 150, 150, 3)\n",
      "Train on 7070 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "7070/7070 [==============================] - 9s 1ms/step - loss: 2.1885 - acc: 0.3603 - val_loss: 3.7998 - val_acc: 0.2951\n",
      "Epoch 2/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 1.7545 - acc: 0.5317 - val_loss: 3.9944 - val_acc: 0.1589\n",
      "Epoch 3/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 1.5177 - acc: 0.6007 - val_loss: 2.0853 - val_acc: 0.3286\n",
      "Epoch 4/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 1.3047 - acc: 0.6658 - val_loss: 1.7401 - val_acc: 0.4719\n",
      "Epoch 5/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 1.1707 - acc: 0.7066 - val_loss: 2.1188 - val_acc: 0.4349\n",
      "Epoch 6/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 1.0617 - acc: 0.7364 - val_loss: 1.5183 - val_acc: 0.5866\n",
      "Epoch 7/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 1.0016 - acc: 0.7501 - val_loss: 1.8031 - val_acc: 0.5161\n",
      "Epoch 8/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.9336 - acc: 0.7767 - val_loss: 1.5621 - val_acc: 0.5699\n",
      "Epoch 9/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.8447 - acc: 0.8035 - val_loss: 1.6732 - val_acc: 0.5866\n",
      "Epoch 10/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.8031 - acc: 0.8144 - val_loss: 1.3776 - val_acc: 0.6416\n",
      "Epoch 11/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.7165 - acc: 0.8399 - val_loss: 1.9373 - val_acc: 0.4803\n",
      "Epoch 12/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.6906 - acc: 0.8443 - val_loss: 1.7000 - val_acc: 0.5615\n",
      "Epoch 13/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.6516 - acc: 0.8535 - val_loss: 1.6325 - val_acc: 0.5293\n",
      "Epoch 14/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.6278 - acc: 0.8670 - val_loss: 1.5185 - val_acc: 0.5663\n",
      "Epoch 15/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.5778 - acc: 0.8801 - val_loss: 1.4352 - val_acc: 0.6225\n",
      "Epoch 16/100\n",
      "6912/7070 [============================>.] - ETA: 0s - loss: 0.5401 - acc: 0.8909\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.5419 - acc: 0.8902 - val_loss: 1.3987 - val_acc: 0.6535\n",
      "Epoch 17/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.4606 - acc: 0.9177 - val_loss: 1.0705 - val_acc: 0.7324\n",
      "Epoch 18/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.4170 - acc: 0.9335 - val_loss: 1.0516 - val_acc: 0.7204\n",
      "Epoch 19/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3936 - acc: 0.9407 - val_loss: 1.0238 - val_acc: 0.7228\n",
      "Epoch 20/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3791 - acc: 0.9482 - val_loss: 0.9310 - val_acc: 0.7563\n",
      "Epoch 21/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3754 - acc: 0.9477 - val_loss: 0.9602 - val_acc: 0.7503\n",
      "Epoch 22/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3623 - acc: 0.9491 - val_loss: 0.8703 - val_acc: 0.7838\n",
      "Epoch 23/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3536 - acc: 0.9495 - val_loss: 0.8509 - val_acc: 0.7909\n",
      "Epoch 24/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3439 - acc: 0.9560 - val_loss: 0.9179 - val_acc: 0.7575\n",
      "Epoch 25/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3356 - acc: 0.9552 - val_loss: 0.8925 - val_acc: 0.7658\n",
      "Epoch 26/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3325 - acc: 0.9556 - val_loss: 0.9621 - val_acc: 0.7467\n",
      "Epoch 27/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3306 - acc: 0.9550 - val_loss: 0.9384 - val_acc: 0.7372\n",
      "Epoch 28/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3130 - acc: 0.9605 - val_loss: 0.9497 - val_acc: 0.7276\n",
      "Epoch 29/100\n",
      "6912/7070 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9593\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.3098 - acc: 0.9595 - val_loss: 0.8828 - val_acc: 0.7539\n",
      "Epoch 30/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2932 - acc: 0.9644 - val_loss: 0.8331 - val_acc: 0.7599\n",
      "Epoch 31/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2898 - acc: 0.9679 - val_loss: 0.7943 - val_acc: 0.7790\n",
      "Epoch 32/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2824 - acc: 0.9716 - val_loss: 0.7810 - val_acc: 0.7849\n",
      "Epoch 33/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2867 - acc: 0.9687 - val_loss: 0.7740 - val_acc: 0.7885\n",
      "Epoch 34/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2853 - acc: 0.9680 - val_loss: 0.7574 - val_acc: 0.8017\n",
      "Epoch 35/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2842 - acc: 0.9694 - val_loss: 0.7598 - val_acc: 0.8005\n",
      "Epoch 36/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2826 - acc: 0.9683 - val_loss: 0.7652 - val_acc: 0.8029\n",
      "Epoch 37/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2804 - acc: 0.9707 - val_loss: 0.7544 - val_acc: 0.7993\n",
      "Epoch 38/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2856 - acc: 0.9675 - val_loss: 0.7541 - val_acc: 0.8029\n",
      "Epoch 39/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2822 - acc: 0.9686 - val_loss: 0.7595 - val_acc: 0.8005\n",
      "Epoch 40/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2738 - acc: 0.9751 - val_loss: 0.7573 - val_acc: 0.8017\n",
      "Epoch 41/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2753 - acc: 0.9720 - val_loss: 0.7505 - val_acc: 0.8041\n",
      "Epoch 42/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2784 - acc: 0.9689 - val_loss: 0.7452 - val_acc: 0.8112\n",
      "Epoch 43/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2767 - acc: 0.9720 - val_loss: 0.7478 - val_acc: 0.8112\n",
      "Epoch 44/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2747 - acc: 0.9724 - val_loss: 0.7397 - val_acc: 0.8065\n",
      "Epoch 45/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2799 - acc: 0.9683 - val_loss: 0.7568 - val_acc: 0.7957\n",
      "Epoch 46/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2752 - acc: 0.9706 - val_loss: 0.7446 - val_acc: 0.8076\n",
      "Epoch 47/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2720 - acc: 0.9734 - val_loss: 0.7541 - val_acc: 0.8005\n",
      "Epoch 48/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2704 - acc: 0.9707 - val_loss: 0.7461 - val_acc: 0.8076\n",
      "Epoch 49/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2661 - acc: 0.9750 - val_loss: 0.7351 - val_acc: 0.8124\n",
      "Epoch 50/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2666 - acc: 0.9733 - val_loss: 0.7475 - val_acc: 0.8053\n",
      "Epoch 51/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2656 - acc: 0.9752 - val_loss: 0.7306 - val_acc: 0.8065\n",
      "Epoch 52/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2655 - acc: 0.9741 - val_loss: 0.7398 - val_acc: 0.8088\n",
      "Epoch 53/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2672 - acc: 0.9723 - val_loss: 0.7447 - val_acc: 0.8053\n",
      "Epoch 54/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2664 - acc: 0.9710 - val_loss: 0.7713 - val_acc: 0.7933\n",
      "Epoch 55/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2654 - acc: 0.9747 - val_loss: 0.7511 - val_acc: 0.8005\n",
      "Epoch 56/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2642 - acc: 0.9741 - val_loss: 0.7367 - val_acc: 0.8041\n",
      "Epoch 57/100\n",
      "6912/7070 [============================>.] - ETA: 0s - loss: 0.2620 - acc: 0.9727\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2615 - acc: 0.9727 - val_loss: 0.7453 - val_acc: 0.7969\n",
      "Epoch 58/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2637 - acc: 0.9727 - val_loss: 0.7431 - val_acc: 0.8005\n",
      "Epoch 59/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2647 - acc: 0.9743 - val_loss: 0.7391 - val_acc: 0.8029\n",
      "Epoch 60/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2636 - acc: 0.9740 - val_loss: 0.7389 - val_acc: 0.8041\n",
      "Epoch 61/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2689 - acc: 0.9723 - val_loss: 0.7420 - val_acc: 0.8065\n",
      "Epoch 62/100\n",
      "6912/7070 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9725\n",
      "Epoch 00062: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2621 - acc: 0.9723 - val_loss: 0.7420 - val_acc: 0.8053\n",
      "Epoch 63/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2576 - acc: 0.9757 - val_loss: 0.7414 - val_acc: 0.8053\n",
      "Epoch 64/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2609 - acc: 0.9724 - val_loss: 0.7412 - val_acc: 0.8053\n",
      "Epoch 65/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2650 - acc: 0.9740 - val_loss: 0.7411 - val_acc: 0.8053\n",
      "Epoch 66/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2619 - acc: 0.9747 - val_loss: 0.7414 - val_acc: 0.8053\n",
      "Epoch 67/100\n",
      "6912/7070 [============================>.] - ETA: 0s - loss: 0.2564 - acc: 0.9764\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2562 - acc: 0.9765 - val_loss: 0.7409 - val_acc: 0.8041\n",
      "Epoch 68/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2623 - acc: 0.9743 - val_loss: 0.7411 - val_acc: 0.8041\n",
      "Epoch 69/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2593 - acc: 0.9757 - val_loss: 0.7412 - val_acc: 0.8041\n",
      "Epoch 70/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2570 - acc: 0.9765 - val_loss: 0.7413 - val_acc: 0.8053\n",
      "Epoch 71/100\n",
      "7070/7070 [==============================] - 8s 1ms/step - loss: 0.2597 - acc: 0.9764 - val_loss: 0.7412 - val_acc: 0.8041\n",
      "Epoch 00071: early stopping\n",
      "823/823 [==============================] - 0s 442us/step\n",
      "\n",
      "Accuracy = 0.7400\n",
      "\n",
      "Error Rate = 0.2600\n",
      "F-Score = {:.4f} 0.739975698663\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.982012510299683\n",
      "Scaling time: 1.1725635528564453\n",
      "Scaling time: 1.1706047058105469\n",
      "(7055, 150, 150, 3)\n",
      "Train on 7055 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "7055/7055 [==============================] - 9s 1ms/step - loss: 2.1908 - acc: 0.3559 - val_loss: 2.8461 - val_acc: 0.2712\n",
      "Epoch 2/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 1.7577 - acc: 0.5219 - val_loss: 2.4567 - val_acc: 0.2784\n",
      "Epoch 3/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 1.5394 - acc: 0.5861 - val_loss: 1.9932 - val_acc: 0.3632\n",
      "Epoch 4/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 1.3153 - acc: 0.6662 - val_loss: 2.1148 - val_acc: 0.3524\n",
      "Epoch 5/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 1.1945 - acc: 0.6974 - val_loss: 1.7108 - val_acc: 0.5030\n",
      "Epoch 6/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 1.0808 - acc: 0.7315 - val_loss: 1.8251 - val_acc: 0.4922\n",
      "Epoch 7/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.9921 - acc: 0.7589 - val_loss: 1.9260 - val_acc: 0.4170\n",
      "Epoch 8/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.9548 - acc: 0.7648 - val_loss: 1.6050 - val_acc: 0.5269\n",
      "Epoch 9/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.8660 - acc: 0.7916 - val_loss: 1.5538 - val_acc: 0.5305\n",
      "Epoch 10/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.8213 - acc: 0.8065 - val_loss: 2.0541 - val_acc: 0.3978\n",
      "Epoch 11/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.7715 - acc: 0.8189 - val_loss: 1.5427 - val_acc: 0.5520\n",
      "Epoch 12/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.7126 - acc: 0.8360 - val_loss: 1.7164 - val_acc: 0.4671\n",
      "Epoch 13/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.6701 - acc: 0.8498 - val_loss: 1.2482 - val_acc: 0.6344\n",
      "Epoch 14/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.6176 - acc: 0.8680 - val_loss: 1.8823 - val_acc: 0.5078\n",
      "Epoch 15/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.5926 - acc: 0.8740 - val_loss: 1.2783 - val_acc: 0.6225\n",
      "Epoch 16/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.5491 - acc: 0.8875 - val_loss: 1.7574 - val_acc: 0.5030\n",
      "Epoch 17/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.5329 - acc: 0.8943 - val_loss: 1.7525 - val_acc: 0.5352\n",
      "Epoch 18/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.4879 - acc: 0.9022 - val_loss: 1.2709 - val_acc: 0.6440\n",
      "Epoch 19/100\n",
      "6912/7055 [============================>.] - ETA: 0s - loss: 0.4805 - acc: 0.9074\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.4808 - acc: 0.9077 - val_loss: 1.4233 - val_acc: 0.6057\n",
      "Epoch 20/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.4155 - acc: 0.9281 - val_loss: 0.9870 - val_acc: 0.7491\n",
      "Epoch 21/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.3534 - acc: 0.9501 - val_loss: 0.8088 - val_acc: 0.7909\n",
      "Epoch 22/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.3342 - acc: 0.9582 - val_loss: 0.8317 - val_acc: 0.7873\n",
      "Epoch 23/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.3214 - acc: 0.9602 - val_loss: 0.8015 - val_acc: 0.7921\n",
      "Epoch 24/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.3108 - acc: 0.9633 - val_loss: 0.7931 - val_acc: 0.7897\n",
      "Epoch 25/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.3089 - acc: 0.9639 - val_loss: 0.8130 - val_acc: 0.7897\n",
      "Epoch 26/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2994 - acc: 0.9629 - val_loss: 0.8057 - val_acc: 0.7861\n",
      "Epoch 27/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2943 - acc: 0.9644 - val_loss: 0.7834 - val_acc: 0.7969\n",
      "Epoch 28/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2839 - acc: 0.9700 - val_loss: 0.8097 - val_acc: 0.7826\n",
      "Epoch 29/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2745 - acc: 0.9708 - val_loss: 0.8010 - val_acc: 0.7921\n",
      "Epoch 30/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2683 - acc: 0.9714 - val_loss: 0.7692 - val_acc: 0.8076\n",
      "Epoch 31/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2633 - acc: 0.9736 - val_loss: 0.8083 - val_acc: 0.7897\n",
      "Epoch 32/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2625 - acc: 0.9717 - val_loss: 0.8336 - val_acc: 0.7718\n",
      "Epoch 33/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2519 - acc: 0.9763 - val_loss: 0.7474 - val_acc: 0.8124\n",
      "Epoch 34/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2505 - acc: 0.9742 - val_loss: 0.7873 - val_acc: 0.7945\n",
      "Epoch 35/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2384 - acc: 0.9789 - val_loss: 0.7497 - val_acc: 0.8112\n",
      "Epoch 36/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2344 - acc: 0.9786 - val_loss: 0.7464 - val_acc: 0.8244\n",
      "Epoch 37/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2307 - acc: 0.9789 - val_loss: 0.8024 - val_acc: 0.7981\n",
      "Epoch 38/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2266 - acc: 0.9803 - val_loss: 0.7833 - val_acc: 0.8041\n",
      "Epoch 39/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2218 - acc: 0.9793 - val_loss: 0.6829 - val_acc: 0.8315\n",
      "Epoch 40/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2190 - acc: 0.9824 - val_loss: 0.7246 - val_acc: 0.8196\n",
      "Epoch 41/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2080 - acc: 0.9857 - val_loss: 0.8122 - val_acc: 0.8112\n",
      "Epoch 42/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2043 - acc: 0.9845 - val_loss: 0.7313 - val_acc: 0.8148\n",
      "Epoch 43/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.2022 - acc: 0.9845 - val_loss: 0.7459 - val_acc: 0.8148\n",
      "Epoch 44/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1986 - acc: 0.9853 - val_loss: 0.6825 - val_acc: 0.8268\n",
      "Epoch 45/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1950 - acc: 0.9865 - val_loss: 0.7499 - val_acc: 0.8172\n",
      "Epoch 46/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1927 - acc: 0.9858 - val_loss: 0.7271 - val_acc: 0.8268\n",
      "Epoch 47/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1877 - acc: 0.9884 - val_loss: 0.7530 - val_acc: 0.8041\n",
      "Epoch 48/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1832 - acc: 0.9881 - val_loss: 0.7653 - val_acc: 0.7969\n",
      "Epoch 49/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1772 - acc: 0.9911 - val_loss: 0.6970 - val_acc: 0.8280\n",
      "Epoch 50/100\n",
      "6912/7055 [============================>.] - ETA: 0s - loss: 0.1737 - acc: 0.9900\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1738 - acc: 0.9898 - val_loss: 0.7163 - val_acc: 0.8208\n",
      "Epoch 51/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1708 - acc: 0.9908 - val_loss: 0.6901 - val_acc: 0.8303\n",
      "Epoch 52/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1595 - acc: 0.9933 - val_loss: 0.6715 - val_acc: 0.8435\n",
      "Epoch 53/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1611 - acc: 0.9942 - val_loss: 0.6792 - val_acc: 0.8399\n",
      "Epoch 54/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1583 - acc: 0.9953 - val_loss: 0.6709 - val_acc: 0.8411\n",
      "Epoch 55/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1610 - acc: 0.9931 - val_loss: 0.6774 - val_acc: 0.8411\n",
      "Epoch 56/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1602 - acc: 0.9942 - val_loss: 0.6858 - val_acc: 0.8363\n",
      "Epoch 57/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1604 - acc: 0.9935 - val_loss: 0.6796 - val_acc: 0.8423\n",
      "Epoch 58/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1591 - acc: 0.9931 - val_loss: 0.6748 - val_acc: 0.8423\n",
      "Epoch 59/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1598 - acc: 0.9938 - val_loss: 0.6764 - val_acc: 0.8399\n",
      "Epoch 60/100\n",
      "6912/7055 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.9945\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1568 - acc: 0.9946 - val_loss: 0.6796 - val_acc: 0.8411\n",
      "Epoch 61/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1562 - acc: 0.9953 - val_loss: 0.6778 - val_acc: 0.8411\n",
      "Epoch 62/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1542 - acc: 0.9953 - val_loss: 0.6777 - val_acc: 0.8435\n",
      "Epoch 63/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1570 - acc: 0.9938 - val_loss: 0.6779 - val_acc: 0.8435\n",
      "Epoch 64/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1545 - acc: 0.9949 - val_loss: 0.6758 - val_acc: 0.8435\n",
      "Epoch 65/100\n",
      "6912/7055 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9935\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1579 - acc: 0.9932 - val_loss: 0.6760 - val_acc: 0.8435\n",
      "Epoch 66/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1558 - acc: 0.9940 - val_loss: 0.6761 - val_acc: 0.8435\n",
      "Epoch 67/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1553 - acc: 0.9935 - val_loss: 0.6759 - val_acc: 0.8435\n",
      "Epoch 68/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1551 - acc: 0.9949 - val_loss: 0.6760 - val_acc: 0.8447\n",
      "Epoch 69/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1563 - acc: 0.9948 - val_loss: 0.6752 - val_acc: 0.8447\n",
      "Epoch 70/100\n",
      "6912/7055 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.9941\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1576 - acc: 0.9939 - val_loss: 0.6753 - val_acc: 0.8447\n",
      "Epoch 71/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1551 - acc: 0.9952 - val_loss: 0.6751 - val_acc: 0.8471\n",
      "Epoch 72/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1573 - acc: 0.9936 - val_loss: 0.6746 - val_acc: 0.8471\n",
      "Epoch 73/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1546 - acc: 0.9959 - val_loss: 0.6747 - val_acc: 0.8459\n",
      "Epoch 74/100\n",
      "7055/7055 [==============================] - 8s 1ms/step - loss: 0.1568 - acc: 0.9949 - val_loss: 0.6746 - val_acc: 0.8459\n",
      "Epoch 00074: early stopping\n",
      "838/838 [==============================] - 0s 444us/step\n",
      "\n",
      "Accuracy = 0.7709\n",
      "\n",
      "Error Rate = 0.2291\n",
      "F-Score = {:.4f} 0.770883054893\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.766157150268555\n",
      "Scaling time: 1.1628012657165527\n",
      "Scaling time: 1.141113042831421\n",
      "(7087, 150, 150, 3)\n",
      "Train on 7087 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "7087/7087 [==============================] - 9s 1ms/step - loss: 2.2213 - acc: 0.3546 - val_loss: 2.4149 - val_acc: 0.2963\n",
      "Epoch 2/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 1.7906 - acc: 0.5071 - val_loss: 2.1605 - val_acc: 0.3130\n",
      "Epoch 3/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 1.5216 - acc: 0.5995 - val_loss: 2.8050 - val_acc: 0.2461\n",
      "Epoch 4/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 1.3301 - acc: 0.6622 - val_loss: 2.1952 - val_acc: 0.3453\n",
      "Epoch 5/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 1.2052 - acc: 0.6934 - val_loss: 1.9884 - val_acc: 0.4205\n",
      "Epoch 6/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 1.0914 - acc: 0.7244 - val_loss: 2.1484 - val_acc: 0.3477\n",
      "Epoch 7/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.9916 - acc: 0.7572 - val_loss: 1.5244 - val_acc: 0.5460\n",
      "Epoch 8/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.9257 - acc: 0.7740 - val_loss: 1.8242 - val_acc: 0.4588\n",
      "Epoch 9/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.8773 - acc: 0.7847 - val_loss: 1.6876 - val_acc: 0.5054\n",
      "Epoch 10/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.8377 - acc: 0.8005 - val_loss: 1.3375 - val_acc: 0.6177\n",
      "Epoch 11/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.7588 - acc: 0.8291 - val_loss: 1.4158 - val_acc: 0.5902\n",
      "Epoch 12/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.7070 - acc: 0.8379 - val_loss: 1.8846 - val_acc: 0.5293\n",
      "Epoch 13/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.6846 - acc: 0.8424 - val_loss: 1.1983 - val_acc: 0.6822\n",
      "Epoch 14/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.6093 - acc: 0.8706 - val_loss: 1.3578 - val_acc: 0.6165\n",
      "Epoch 15/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.6007 - acc: 0.8706 - val_loss: 1.3617 - val_acc: 0.5842\n",
      "Epoch 16/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.5643 - acc: 0.8816 - val_loss: 1.0456 - val_acc: 0.7145\n",
      "Epoch 17/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.5426 - acc: 0.8881 - val_loss: 1.2216 - val_acc: 0.6583\n",
      "Epoch 18/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.5525 - acc: 0.8813 - val_loss: 1.0361 - val_acc: 0.7109\n",
      "Epoch 19/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.5142 - acc: 0.8993 - val_loss: 1.2304 - val_acc: 0.6452\n",
      "Epoch 20/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.4988 - acc: 0.8993 - val_loss: 2.1196 - val_acc: 0.4648\n",
      "Epoch 21/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.4876 - acc: 0.9029 - val_loss: 1.4158 - val_acc: 0.5902\n",
      "Epoch 22/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.4626 - acc: 0.9076 - val_loss: 1.2692 - val_acc: 0.6499\n",
      "Epoch 23/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.4295 - acc: 0.9214 - val_loss: 1.1631 - val_acc: 0.6930\n",
      "Epoch 24/100\n",
      "6912/7087 [============================>.] - ETA: 0s - loss: 0.3850 - acc: 0.9362\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.3858 - acc: 0.9354 - val_loss: 1.2041 - val_acc: 0.6726\n",
      "Epoch 25/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.3132 - acc: 0.9580 - val_loss: 0.9567 - val_acc: 0.7479\n",
      "Epoch 26/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2896 - acc: 0.9668 - val_loss: 0.8820 - val_acc: 0.7563\n",
      "Epoch 27/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2819 - acc: 0.9647 - val_loss: 0.8684 - val_acc: 0.7706\n",
      "Epoch 28/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2674 - acc: 0.9711 - val_loss: 0.8287 - val_acc: 0.7682\n",
      "Epoch 29/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2600 - acc: 0.9763 - val_loss: 0.8850 - val_acc: 0.7491\n",
      "Epoch 30/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2527 - acc: 0.9756 - val_loss: 0.8257 - val_acc: 0.7814\n",
      "Epoch 31/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2482 - acc: 0.9764 - val_loss: 0.8344 - val_acc: 0.7599\n",
      "Epoch 32/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2448 - acc: 0.9759 - val_loss: 0.8023 - val_acc: 0.7861\n",
      "Epoch 33/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2397 - acc: 0.9766 - val_loss: 0.8136 - val_acc: 0.7718\n",
      "Epoch 34/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2348 - acc: 0.9786 - val_loss: 0.7773 - val_acc: 0.7909\n",
      "Epoch 35/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2282 - acc: 0.9815 - val_loss: 0.8161 - val_acc: 0.7790\n",
      "Epoch 36/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2222 - acc: 0.9805 - val_loss: 0.8201 - val_acc: 0.7634\n",
      "Epoch 37/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2188 - acc: 0.9811 - val_loss: 0.7701 - val_acc: 0.7897\n",
      "Epoch 38/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2171 - acc: 0.9804 - val_loss: 0.7792 - val_acc: 0.7897\n",
      "Epoch 39/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2140 - acc: 0.9819 - val_loss: 0.8376 - val_acc: 0.7694\n",
      "Epoch 40/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2055 - acc: 0.9832 - val_loss: 0.7732 - val_acc: 0.7933\n",
      "Epoch 41/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.2011 - acc: 0.9850 - val_loss: 0.7912 - val_acc: 0.7802\n",
      "Epoch 42/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1974 - acc: 0.9855 - val_loss: 0.7798 - val_acc: 0.7885\n",
      "Epoch 43/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1947 - acc: 0.9845 - val_loss: 0.7522 - val_acc: 0.7897\n",
      "Epoch 44/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1873 - acc: 0.9890 - val_loss: 0.7585 - val_acc: 0.7993\n",
      "Epoch 45/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1866 - acc: 0.9869 - val_loss: 0.6982 - val_acc: 0.8196\n",
      "Epoch 46/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1849 - acc: 0.9883 - val_loss: 0.7584 - val_acc: 0.8076\n",
      "Epoch 47/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1796 - acc: 0.9891 - val_loss: 0.7134 - val_acc: 0.8088\n",
      "Epoch 48/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1776 - acc: 0.9877 - val_loss: 0.7124 - val_acc: 0.8148\n",
      "Epoch 49/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1738 - acc: 0.9881 - val_loss: 0.7733 - val_acc: 0.7849\n",
      "Epoch 50/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1719 - acc: 0.9876 - val_loss: 0.7636 - val_acc: 0.8148\n",
      "Epoch 51/100\n",
      "6912/7087 [============================>.] - ETA: 0s - loss: 0.1683 - acc: 0.9884\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1680 - acc: 0.9886 - val_loss: 0.7604 - val_acc: 0.8088\n",
      "Epoch 52/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1629 - acc: 0.9908 - val_loss: 0.7296 - val_acc: 0.8136\n",
      "Epoch 53/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1553 - acc: 0.9934 - val_loss: 0.7171 - val_acc: 0.8136\n",
      "Epoch 54/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1546 - acc: 0.9937 - val_loss: 0.6898 - val_acc: 0.8232\n",
      "Epoch 55/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1557 - acc: 0.9932 - val_loss: 0.6816 - val_acc: 0.8280\n",
      "Epoch 56/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1534 - acc: 0.9929 - val_loss: 0.6975 - val_acc: 0.8172\n",
      "Epoch 57/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1529 - acc: 0.9931 - val_loss: 0.6912 - val_acc: 0.8136\n",
      "Epoch 58/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1523 - acc: 0.9939 - val_loss: 0.6966 - val_acc: 0.8112\n",
      "Epoch 59/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1519 - acc: 0.9942 - val_loss: 0.6923 - val_acc: 0.8172\n",
      "Epoch 60/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1515 - acc: 0.9942 - val_loss: 0.6881 - val_acc: 0.8124\n",
      "Epoch 61/100\n",
      "6912/7087 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9948\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1491 - acc: 0.9948 - val_loss: 0.6862 - val_acc: 0.8136\n",
      "Epoch 62/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1524 - acc: 0.9942 - val_loss: 0.6881 - val_acc: 0.8124\n",
      "Epoch 63/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1513 - acc: 0.9935 - val_loss: 0.6898 - val_acc: 0.8124\n",
      "Epoch 64/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1502 - acc: 0.9941 - val_loss: 0.6897 - val_acc: 0.8124\n",
      "Epoch 65/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1503 - acc: 0.9946 - val_loss: 0.6898 - val_acc: 0.8100\n",
      "Epoch 66/100\n",
      "6912/7087 [============================>.] - ETA: 0s - loss: 0.1511 - acc: 0.9926\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1509 - acc: 0.9928 - val_loss: 0.6905 - val_acc: 0.8112\n",
      "Epoch 67/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1513 - acc: 0.9935 - val_loss: 0.6907 - val_acc: 0.8100\n",
      "Epoch 68/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1510 - acc: 0.9939 - val_loss: 0.6903 - val_acc: 0.8088\n",
      "Epoch 69/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1501 - acc: 0.9945 - val_loss: 0.6903 - val_acc: 0.8088\n",
      "Epoch 70/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1507 - acc: 0.9939 - val_loss: 0.6902 - val_acc: 0.8088\n",
      "Epoch 71/100\n",
      "6912/7087 [============================>.] - ETA: 0s - loss: 0.1509 - acc: 0.9936\n",
      "Epoch 00071: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1506 - acc: 0.9938 - val_loss: 0.6903 - val_acc: 0.8100\n",
      "Epoch 72/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1499 - acc: 0.9939 - val_loss: 0.6902 - val_acc: 0.8100\n",
      "Epoch 73/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1496 - acc: 0.9953 - val_loss: 0.6900 - val_acc: 0.8100\n",
      "Epoch 74/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1505 - acc: 0.9935 - val_loss: 0.6905 - val_acc: 0.8100\n",
      "Epoch 75/100\n",
      "7087/7087 [==============================] - 8s 1ms/step - loss: 0.1480 - acc: 0.9952 - val_loss: 0.6906 - val_acc: 0.8100\n",
      "Epoch 00075: early stopping\n",
      "806/806 [==============================] - 0s 450us/step\n",
      "\n",
      "Accuracy = 0.7556\n",
      "\n",
      "Error Rate = 0.2444\n",
      "F-Score = {:.4f} 0.755583126551\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Scaling time: 10.729100227355957\n",
      "Scaling time: 1.335644245147705\n",
      "Scaling time: 1.2116925716400146\n",
      "(7077, 150, 150, 3)\n",
      "Train on 7077 samples, validate on 837 samples\n",
      "Epoch 1/100\n",
      "7077/7077 [==============================] - 10s 1ms/step - loss: 2.2176 - acc: 0.3288 - val_loss: 2.9746 - val_acc: 0.2616\n",
      "Epoch 2/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.8080 - acc: 0.4850 - val_loss: 2.6725 - val_acc: 0.2605\n",
      "Epoch 3/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.5226 - acc: 0.5816 - val_loss: 2.9634 - val_acc: 0.2401\n",
      "Epoch 4/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.3434 - acc: 0.6470 - val_loss: 2.6770 - val_acc: 0.2234\n",
      "Epoch 5/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.2002 - acc: 0.6944 - val_loss: 1.9644 - val_acc: 0.3751\n",
      "Epoch 6/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.0921 - acc: 0.7270 - val_loss: 1.7702 - val_acc: 0.4815\n",
      "Epoch 7/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.9973 - acc: 0.7488 - val_loss: 2.1539 - val_acc: 0.3943\n",
      "Epoch 8/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.9023 - acc: 0.7888 - val_loss: 2.1471 - val_acc: 0.3859\n",
      "Epoch 9/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.8273 - acc: 0.8091 - val_loss: 1.5666 - val_acc: 0.5675\n",
      "Epoch 10/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.7902 - acc: 0.8183 - val_loss: 1.3637 - val_acc: 0.5962\n",
      "Epoch 11/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.7130 - acc: 0.8446 - val_loss: 1.3754 - val_acc: 0.6476\n",
      "Epoch 12/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.6515 - acc: 0.8607 - val_loss: 1.8060 - val_acc: 0.5125\n",
      "Epoch 13/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.6428 - acc: 0.8629 - val_loss: 1.4270 - val_acc: 0.5926\n",
      "Epoch 14/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.5865 - acc: 0.8764 - val_loss: 1.5202 - val_acc: 0.6165\n",
      "Epoch 15/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.5566 - acc: 0.8878 - val_loss: 1.3816 - val_acc: 0.6607\n",
      "Epoch 16/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.5026 - acc: 0.9045\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.5015 - acc: 0.9052 - val_loss: 1.5023 - val_acc: 0.6057\n",
      "Epoch 17/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.4198 - acc: 0.9289 - val_loss: 1.0309 - val_acc: 0.7192\n",
      "Epoch 18/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3903 - acc: 0.9401 - val_loss: 0.9922 - val_acc: 0.7515\n",
      "Epoch 19/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3691 - acc: 0.9460 - val_loss: 0.9385 - val_acc: 0.7706\n",
      "Epoch 20/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3527 - acc: 0.9513 - val_loss: 0.9178 - val_acc: 0.7706\n",
      "Epoch 21/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3538 - acc: 0.9496 - val_loss: 0.8574 - val_acc: 0.7742\n",
      "Epoch 22/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3389 - acc: 0.9505 - val_loss: 0.7623 - val_acc: 0.8196\n",
      "Epoch 23/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3340 - acc: 0.9546 - val_loss: 0.7647 - val_acc: 0.8172\n",
      "Epoch 24/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3220 - acc: 0.9555 - val_loss: 0.7705 - val_acc: 0.8124\n",
      "Epoch 25/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3171 - acc: 0.9576 - val_loss: 0.7584 - val_acc: 0.8100\n",
      "Epoch 26/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3089 - acc: 0.9594 - val_loss: 0.7825 - val_acc: 0.8005\n",
      "Epoch 27/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3051 - acc: 0.9627 - val_loss: 0.7290 - val_acc: 0.8148\n",
      "Epoch 28/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2990 - acc: 0.9614 - val_loss: 0.7522 - val_acc: 0.7993\n",
      "Epoch 29/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2835 - acc: 0.9665 - val_loss: 0.7200 - val_acc: 0.8184\n",
      "Epoch 30/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2844 - acc: 0.9645 - val_loss: 0.7361 - val_acc: 0.8041\n",
      "Epoch 31/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2844 - acc: 0.9633 - val_loss: 0.7400 - val_acc: 0.8184\n",
      "Epoch 32/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2681 - acc: 0.9654 - val_loss: 0.7286 - val_acc: 0.8172\n",
      "Epoch 33/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2586 - acc: 0.9734 - val_loss: 0.7732 - val_acc: 0.7957\n",
      "Epoch 34/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2660 - acc: 0.9679 - val_loss: 0.7408 - val_acc: 0.7969\n",
      "Epoch 35/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2465 - acc: 0.9727 - val_loss: 0.6821 - val_acc: 0.8292\n",
      "Epoch 36/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2428 - acc: 0.9747 - val_loss: 0.7379 - val_acc: 0.8088\n",
      "Epoch 37/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2335 - acc: 0.9771 - val_loss: 0.7343 - val_acc: 0.7993\n",
      "Epoch 38/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2362 - acc: 0.9733 - val_loss: 0.7044 - val_acc: 0.8220\n",
      "Epoch 39/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2351 - acc: 0.9753 - val_loss: 0.7052 - val_acc: 0.8280\n",
      "Epoch 40/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2228 - acc: 0.9784 - val_loss: 0.7089 - val_acc: 0.8148\n",
      "Epoch 41/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.2202 - acc: 0.9787\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2206 - acc: 0.9785 - val_loss: 0.7864 - val_acc: 0.7933\n",
      "Epoch 42/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2113 - acc: 0.9806 - val_loss: 0.6637 - val_acc: 0.8303\n",
      "Epoch 43/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2010 - acc: 0.9853 - val_loss: 0.6748 - val_acc: 0.8244\n",
      "Epoch 44/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1974 - acc: 0.9860 - val_loss: 0.6792 - val_acc: 0.8280\n",
      "Epoch 45/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1976 - acc: 0.9880 - val_loss: 0.6692 - val_acc: 0.8315\n",
      "Epoch 46/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1985 - acc: 0.9849 - val_loss: 0.6648 - val_acc: 0.8303\n",
      "Epoch 47/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1936 - acc: 0.9880 - val_loss: 0.6718 - val_acc: 0.8363\n",
      "Epoch 48/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1965 - acc: 0.9874 - val_loss: 0.6569 - val_acc: 0.8399\n",
      "Epoch 49/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1989 - acc: 0.9874 - val_loss: 0.6558 - val_acc: 0.8399\n",
      "Epoch 50/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1944 - acc: 0.9864 - val_loss: 0.6640 - val_acc: 0.8411\n",
      "Epoch 51/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1941 - acc: 0.9874 - val_loss: 0.6731 - val_acc: 0.8363\n",
      "Epoch 52/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1945 - acc: 0.9869 - val_loss: 0.6639 - val_acc: 0.8399\n",
      "Epoch 53/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1940 - acc: 0.9880 - val_loss: 0.6641 - val_acc: 0.8435\n",
      "Epoch 54/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1918 - acc: 0.9874 - val_loss: 0.6626 - val_acc: 0.8411\n",
      "Epoch 55/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.1909 - acc: 0.9881\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1908 - acc: 0.9883 - val_loss: 0.6563 - val_acc: 0.8471\n",
      "Epoch 56/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1882 - acc: 0.9890 - val_loss: 0.6583 - val_acc: 0.8483\n",
      "Epoch 57/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1890 - acc: 0.9878 - val_loss: 0.6606 - val_acc: 0.8471\n",
      "Epoch 58/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1890 - acc: 0.9890 - val_loss: 0.6633 - val_acc: 0.8471\n",
      "Epoch 59/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1863 - acc: 0.9901 - val_loss: 0.6635 - val_acc: 0.8447\n",
      "Epoch 60/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.1866 - acc: 0.9894\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1867 - acc: 0.9893 - val_loss: 0.6618 - val_acc: 0.8447\n",
      "Epoch 61/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1906 - acc: 0.9884 - val_loss: 0.6627 - val_acc: 0.8459\n",
      "Epoch 62/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1923 - acc: 0.9867 - val_loss: 0.6632 - val_acc: 0.8459\n",
      "Epoch 63/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1870 - acc: 0.9894 - val_loss: 0.6642 - val_acc: 0.8447\n",
      "Epoch 64/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1877 - acc: 0.9886 - val_loss: 0.6643 - val_acc: 0.8447\n",
      "Epoch 65/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.1899 - acc: 0.9886\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1902 - acc: 0.9886 - val_loss: 0.6644 - val_acc: 0.8447\n",
      "Epoch 66/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1883 - acc: 0.9880 - val_loss: 0.6646 - val_acc: 0.8447\n",
      "Epoch 67/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1874 - acc: 0.9888 - val_loss: 0.6650 - val_acc: 0.8447\n",
      "Epoch 68/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1881 - acc: 0.9894 - val_loss: 0.6649 - val_acc: 0.8447\n",
      "Epoch 69/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1862 - acc: 0.9894 - val_loss: 0.6651 - val_acc: 0.8447\n",
      "Epoch 00069: early stopping\n",
      "816/816 [==============================] - 0s 450us/step\n",
      "\n",
      "Accuracy = 0.8272\n",
      "\n",
      "Error Rate = 0.1728\n",
      "F-Score = {:.4f} 0.827205882353\n",
      "Adding  fold1 New Features:  (871, 150, 150, 3)\n",
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Scaling time: 10.186474800109863\n",
      "Scaling time: 1.2253634929656982\n",
      "Scaling time: 1.2742669582366943\n",
      "(7077, 150, 150, 3)\n",
      "Train on 7077 samples, validate on 816 samples\n",
      "Epoch 1/100\n",
      "7077/7077 [==============================] - 9s 1ms/step - loss: 2.1549 - acc: 0.3863 - val_loss: 4.0247 - val_acc: 0.1777\n",
      "Epoch 2/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.7508 - acc: 0.5362 - val_loss: 1.9483 - val_acc: 0.4301\n",
      "Epoch 3/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.5051 - acc: 0.6001 - val_loss: 1.7899 - val_acc: 0.4669\n",
      "Epoch 4/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.3188 - acc: 0.6637 - val_loss: 1.7128 - val_acc: 0.5110\n",
      "Epoch 5/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.1744 - acc: 0.6983 - val_loss: 2.2195 - val_acc: 0.3125\n",
      "Epoch 6/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 1.0805 - acc: 0.7331 - val_loss: 2.3014 - val_acc: 0.3811\n",
      "Epoch 7/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.9743 - acc: 0.7677 - val_loss: 1.9644 - val_acc: 0.4240\n",
      "Epoch 8/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.8926 - acc: 0.7890 - val_loss: 1.4342 - val_acc: 0.5846\n",
      "Epoch 9/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.8112 - acc: 0.8157 - val_loss: 1.7853 - val_acc: 0.5208\n",
      "Epoch 10/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.7354 - acc: 0.8382 - val_loss: 1.5346 - val_acc: 0.5551\n",
      "Epoch 11/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.7025 - acc: 0.8419 - val_loss: 1.8994 - val_acc: 0.4890\n",
      "Epoch 12/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.6515 - acc: 0.8615 - val_loss: 1.6161 - val_acc: 0.6103\n",
      "Epoch 13/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.6002 - acc: 0.8734 - val_loss: 1.3045 - val_acc: 0.6336\n",
      "Epoch 14/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.5713 - acc: 0.8833 - val_loss: 1.2124 - val_acc: 0.6556\n",
      "Epoch 15/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.5720 - acc: 0.8779 - val_loss: 1.2146 - val_acc: 0.6593\n",
      "Epoch 16/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.5035 - acc: 0.9017 - val_loss: 1.3804 - val_acc: 0.6213\n",
      "Epoch 17/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.4984 - acc: 0.9036 - val_loss: 0.9498 - val_acc: 0.7353\n",
      "Epoch 18/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.4716 - acc: 0.9128 - val_loss: 1.6181 - val_acc: 0.5343\n",
      "Epoch 19/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.4475 - acc: 0.9179 - val_loss: 1.3058 - val_acc: 0.6887\n",
      "Epoch 20/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.4177 - acc: 0.9260 - val_loss: 1.2347 - val_acc: 0.6691\n",
      "Epoch 21/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.4125 - acc: 0.9284 - val_loss: 1.2613 - val_acc: 0.6814\n",
      "Epoch 22/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3920 - acc: 0.9333 - val_loss: 1.0153 - val_acc: 0.7304\n",
      "Epoch 23/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.3707 - acc: 0.9391\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3723 - acc: 0.9385 - val_loss: 1.3345 - val_acc: 0.6703\n",
      "Epoch 24/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.3123 - acc: 0.9582 - val_loss: 0.9048 - val_acc: 0.7929\n",
      "Epoch 25/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2737 - acc: 0.9698 - val_loss: 0.8363 - val_acc: 0.7978\n",
      "Epoch 26/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2525 - acc: 0.9774 - val_loss: 0.8279 - val_acc: 0.8027\n",
      "Epoch 27/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2492 - acc: 0.9770 - val_loss: 0.7742 - val_acc: 0.8125\n",
      "Epoch 28/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2426 - acc: 0.9770 - val_loss: 0.7740 - val_acc: 0.8150\n",
      "Epoch 29/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2336 - acc: 0.9823 - val_loss: 0.8367 - val_acc: 0.7978\n",
      "Epoch 30/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2295 - acc: 0.9809 - val_loss: 0.7567 - val_acc: 0.8186\n",
      "Epoch 31/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2235 - acc: 0.9830 - val_loss: 0.7686 - val_acc: 0.8199\n",
      "Epoch 32/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2170 - acc: 0.9838 - val_loss: 0.7648 - val_acc: 0.8186\n",
      "Epoch 33/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2158 - acc: 0.9812 - val_loss: 0.7516 - val_acc: 0.8174\n",
      "Epoch 34/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2072 - acc: 0.9845 - val_loss: 0.7974 - val_acc: 0.8076\n",
      "Epoch 35/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.2034 - acc: 0.9854 - val_loss: 0.7836 - val_acc: 0.8064\n",
      "Epoch 36/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1968 - acc: 0.9864 - val_loss: 0.8440 - val_acc: 0.7904\n",
      "Epoch 37/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1993 - acc: 0.9852 - val_loss: 0.7406 - val_acc: 0.8272\n",
      "Epoch 38/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1908 - acc: 0.9881 - val_loss: 0.8337 - val_acc: 0.8015\n",
      "Epoch 39/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1885 - acc: 0.9883 - val_loss: 0.7647 - val_acc: 0.8235\n",
      "Epoch 40/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1802 - acc: 0.9895 - val_loss: 0.7191 - val_acc: 0.8272\n",
      "Epoch 41/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1800 - acc: 0.9897 - val_loss: 0.7988 - val_acc: 0.8125\n",
      "Epoch 42/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1782 - acc: 0.9881 - val_loss: 0.8259 - val_acc: 0.8137\n",
      "Epoch 43/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1743 - acc: 0.9898 - val_loss: 0.8157 - val_acc: 0.8223\n",
      "Epoch 44/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1710 - acc: 0.9893 - val_loss: 0.7755 - val_acc: 0.8211\n",
      "Epoch 45/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1687 - acc: 0.9886 - val_loss: 0.7596 - val_acc: 0.8321\n",
      "Epoch 46/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.1642 - acc: 0.9900\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1646 - acc: 0.9900 - val_loss: 0.7756 - val_acc: 0.8076\n",
      "Epoch 47/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1556 - acc: 0.9931 - val_loss: 0.7710 - val_acc: 0.8284\n",
      "Epoch 48/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1529 - acc: 0.9935 - val_loss: 0.7695 - val_acc: 0.8346\n",
      "Epoch 49/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1532 - acc: 0.9935 - val_loss: 0.7672 - val_acc: 0.8382\n",
      "Epoch 50/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1502 - acc: 0.9938 - val_loss: 0.7771 - val_acc: 0.8358\n",
      "Epoch 51/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.9944\n",
      "Epoch 00051: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1509 - acc: 0.9941 - val_loss: 0.7710 - val_acc: 0.8407\n",
      "Epoch 52/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1491 - acc: 0.9945 - val_loss: 0.7734 - val_acc: 0.8395\n",
      "Epoch 53/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1509 - acc: 0.9939 - val_loss: 0.7739 - val_acc: 0.8395\n",
      "Epoch 54/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1484 - acc: 0.9939 - val_loss: 0.7746 - val_acc: 0.8395\n",
      "Epoch 55/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1490 - acc: 0.9945 - val_loss: 0.7757 - val_acc: 0.8395\n",
      "Epoch 56/100\n",
      "6912/7077 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.9946\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1483 - acc: 0.9942 - val_loss: 0.7760 - val_acc: 0.8395\n",
      "Epoch 57/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1494 - acc: 0.9943 - val_loss: 0.7758 - val_acc: 0.8395\n",
      "Epoch 58/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1478 - acc: 0.9963 - val_loss: 0.7762 - val_acc: 0.8395\n",
      "Epoch 59/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1487 - acc: 0.9939 - val_loss: 0.7769 - val_acc: 0.8395\n",
      "Epoch 60/100\n",
      "7077/7077 [==============================] - 8s 1ms/step - loss: 0.1492 - acc: 0.9942 - val_loss: 0.7779 - val_acc: 0.8395\n",
      "Epoch 00060: early stopping\n",
      "837/837 [==============================] - 0s 449us/step\n",
      "\n",
      "Accuracy = 0.8459\n",
      "\n",
      "Error Rate = 0.1541\n",
      "F-Score = {:.4f} 0.845878136201\n"
     ]
    }
   ],
   "source": [
    "# earlystopping ends training when the validation loss stops improving\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    './sound_classification_epoch_{epoch:03d}_val_loss_{val_loss:.4f}.hdf5',\n",
    "    monitor='val_loss', save_best_only=True)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-7)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "callbacks = [reduce_lr_on_plateau, early_stopping]\n",
    "\n",
    "roc_list = []\n",
    "acc_list = []\n",
    "# preliniary estimation of performance\n",
    "\n",
    "\n",
    "for test_fold in range(1, 11):\n",
    "    keras.backend.clear_session()\n",
    "    model = build_model()\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adamax(0.01))\n",
    "\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = load_all_folds(test_fold)\n",
    "\n",
    "    # for each channel, compute scaling factor\n",
    "    scaler_list = []\n",
    "    (n_clips, n_time, n_freq, n_channel) = train_x.shape\n",
    "\n",
    "    for channel in range(n_channel):\n",
    "        t1 = time.time()\n",
    "        xtrain_2d = train_x[:, :, :, channel].reshape((n_clips * n_time, n_freq))\n",
    "        scaler = sklearn.preprocessing.StandardScaler().fit(xtrain_2d)\n",
    "        # print(\"Channel %d Mean: %s\" % (channel, scaler.mean_,))\n",
    "        # print(\"Channel %d Std: %s\" % (channel, scaler.scale_,))\n",
    "        # print(\"Calculating scaler time: %s\" % (time.time() - t1,))\n",
    "        scaler_list += [scaler]\n",
    "\n",
    "    train_x = do_scale(train_x)\n",
    "    valid_x = do_scale(valid_x)\n",
    "    test_x = do_scale(test_x)\n",
    "    \n",
    "    print(train_x.shape)\n",
    "\n",
    "    # use a batch size to fully utilize GPU power\n",
    "    history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=callbacks,\n",
    "                        batch_size=256,\n",
    "                        epochs=100)\n",
    "    acc = evaluate(model, test_x, test_y)\n",
    "\n",
    "    acc_list += [acc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc mean 0.7832 acc std 0.0463\n"
     ]
    }
   ],
   "source": [
    "acc_array = np.array(acc_list)\n",
    "print(\"acc mean %.4f acc std %.4f\" % (acc_array.mean(), acc_array.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837/837 [==============================] - 0s 443us/step\n",
      "\n",
      "Accuracy = 0.8459\n",
      "\n",
      "Error Rate = 0.1541\n",
      "F-Score = {:.4f} 0.845878136201\n",
      "Showing Confusion Matrix\n",
      "                    air conditioner            horn        children             dog           drill          engine             gun          hammer           siren           music \n",
      "    air conditioner              85               0               7               1               1               5               0               0               1               0 \n",
      "               horn               0              28               0               0               0               0               0               0               0               5 \n",
      "           children               0               0              89               4               1               2               0               0               2               2 \n",
      "                dog               0               1               3              86               3               0               0               2               0               5 \n",
      "              drill               0               2               5               0              69               5               0               5               0              14 \n",
      "             engine               4               0               1               1               0              80               0               2               2               3 \n",
      "                gun               0               0               0               0               0               0              31               1               0               0 \n",
      "             hammer               1               0               0               0               3               4               0              88               0               0 \n",
      "              siren               3               0               4              19               3               0               0               0              54               0 \n",
      "              music               0               0               2               0               0               0               0               0               0              98 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f47d7e52668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz8AAAHVCAYAAADfFUWOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nX9NcBrwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47d7e650f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[earlystop], batch_size=32, nb_epoch=50)\n",
    "acc = evaluate(model, test_x, test_y)  #evaluate(model)\n",
    "\n",
    "labels = [\"air conditioner\", \"horn\", \"children\", \"dog\", \"drill\", \"engine\", \"gun\", \"hammer\", \"siren\", \"music\"]\n",
    "print(\"Showing Confusion Matrix\")\n",
    "y_prob = model.predict(test_x, verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=-1)\n",
    "y_true = np.argmax(test_y, 1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=' ')\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=' ')\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=' ')\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}s\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=' ')\n",
    "        print()\n",
    "\n",
    "print_cm(cm, labels)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, labels, labels)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='g', linewidths=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History keys: dict_keys(['val_loss', 'val_acc', 'loss', 'acc', 'lr'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7MAAAHzCAYAAADy2UoqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiNef/AxxSYaM0C9ZFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47e84ac860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "print(\"History keys:\", (history.history.keys()))\n",
    "# summarise history for training and validation set accuracy\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarise history for training and validation set loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss', fontsize = 'large')\n",
    "plt.xlabel('epoch', fontsize = 'large' )\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Training a more Complex model\n",
    "we will also try to train a more complex model (using Inception-ResNet v2)to see how that performes compared with the relatively shallow CNN network trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding  fold2 New Features:  (888, 150, 150, 3)\n",
      "Adding  fold3 New Features:  (925, 150, 150, 3)\n",
      "Adding  fold4 New Features:  (990, 150, 150, 3)\n",
      "Adding  fold5 New Features:  (936, 150, 150, 3)\n",
      "Adding  fold6 New Features:  (823, 150, 150, 3)\n",
      "Adding  fold7 New Features:  (838, 150, 150, 3)\n",
      "Adding  fold8 New Features:  (806, 150, 150, 3)\n",
      "Adding  fold9 New Features:  (816, 150, 150, 3)\n",
      "Scaling time: 9.650416135787964\n",
      "Scaling time: 1.1975748538970947\n",
      "Scaling time: 1.2602355480194092\n",
      "(7022, 150, 150, 3)\n",
      "Train on 7022 samples, validate on 837 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[256,7,7,1088]\n\t [[Node: block17_17/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block17_17_conv/BiasAdd, block17_1/mul/y)]]\n\t [[Node: loss_1/mul/_5967 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_53978_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'block17_17/mul', defined at:\n  File \"/home/maxkaz/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-4fd5d5475729>\", line 19, in <module>\n    classes=10)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/applications/inception_resnet_v2.py\", line 304, in InceptionResNetV2\n    block_idx=block_idx)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/applications/inception_resnet_v2.py\", line 171, in inception_resnet_block\n    name=block_name)([x, up])\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 663, in call\n    return self.function(inputs, **arguments)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/applications/inception_resnet_v2.py\", line 168, in <lambda>\n    x = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,7,7,1088]\n\t [[Node: block17_17/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block17_17_conv/BiasAdd, block17_1/mul/y)]]\n\t [[Node: loss_1/mul/_5967 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_53978_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[256,7,7,1088]\n\t [[Node: block17_17/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block17_17_conv/BiasAdd, block17_1/mul/y)]]\n\t [[Node: loss_1/mul/_5967 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_53978_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4fd5d5475729>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=callbacks,\n\u001b[1;32m     48\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                         epochs=100)\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[256,7,7,1088]\n\t [[Node: block17_17/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block17_17_conv/BiasAdd, block17_1/mul/y)]]\n\t [[Node: loss_1/mul/_5967 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_53978_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'block17_17/mul', defined at:\n  File \"/home/maxkaz/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-4fd5d5475729>\", line 19, in <module>\n    classes=10)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/applications/inception_resnet_v2.py\", line 304, in InceptionResNetV2\n    block_idx=block_idx)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/applications/inception_resnet_v2.py\", line 171, in inception_resnet_block\n    name=block_name)([x, up])\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\", line 617, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/layers/core.py\", line 663, in call\n    return self.function(inputs, **arguments)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/keras/applications/inception_resnet_v2.py\", line 168, in <lambda>\n    x = Lambda(lambda inputs, scale: inputs[0] + inputs[1] * scale,\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 894, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\", line 1117, in _mul_dispatch\n    return gen_math_ops._mul(x, y, name=name)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 2726, in _mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/home/maxkaz/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,7,7,1088]\n\t [[Node: block17_17/mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](block17_17_conv/BiasAdd, block17_1/mul/y)]]\n\t [[Node: loss_1/mul/_5967 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_53978_loss_1/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# earlystopping ends training when the validation loss stops improving\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    './sound_classification_epoch_{epoch:03d}_val_loss_{val_loss:.4f}.hdf5',\n",
    "    monitor='val_loss', save_best_only=True)\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-7)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1)\n",
    "callbacks = [reduce_lr_on_plateau, early_stopping]\n",
    "\n",
    "roc_list = []\n",
    "acc_list = []\n",
    "# preliniary estimation of performance\n",
    "\n",
    "\n",
    "for test_fold in range(1, 11):\n",
    "    # clear backend session per http://forums.fast.ai/t/how-could-i-release-gpu-memory-of-keras/2023/13\n",
    "    K.clear_session()\n",
    "    print(\"current test fold:\", test_fold)\n",
    "\n",
    "    model = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights=None,\n",
    "                                                                     input_shape=(\n",
    "                                                                         frames, bands, num_channels),\n",
    "                                                                     classes=10)\n",
    "\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adamax(0.01))\n",
    "\n",
    "    train_x, train_y, valid_x, valid_y, test_x, test_y = load_all_folds(test_fold)\n",
    "\n",
    "    # for each channel, compute scaling factor\n",
    "    scaler_list = []\n",
    "    (n_clips, n_time, n_freq, n_channel) = train_x.shape\n",
    "\n",
    "    for channel in range(n_channel):\n",
    "        t1 = time.time()\n",
    "        xtrain_2d = train_x[:, :, :, channel].reshape((n_clips * n_time, n_freq))\n",
    "        scaler = sklearn.preprocessing.StandardScaler().fit(xtrain_2d)\n",
    "        # print(\"Channel %d Mean: %s\" % (channel, scaler.mean_,))\n",
    "        # print(\"Channel %d Std: %s\" % (channel, scaler.scale_,))\n",
    "        # print(\"Calculating scaler time: %s\" % (time.time() - t1,))\n",
    "        scaler_list += [scaler]\n",
    "\n",
    "    train_x = do_scale(train_x)\n",
    "    valid_x = do_scale(valid_x)\n",
    "    test_x = do_scale(test_x)\n",
    "    \n",
    "    print(train_x.shape)\n",
    "\n",
    "    # use a batch size to fully utilize GPU power\n",
    "    history = model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=callbacks,\n",
    "                        batch_size=256,\n",
    "                        epochs=100)\n",
    "    acc = evaluate(model, test_x, test_y)\n",
    "\n",
    "    acc_list += [acc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "acc_array = np.array(acc_list)\n",
    "print(\"acc mean %.4f acc std %.4f\" % (acc_array.mean(), acc_array.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#model.fit(train_x, train_y, validation_data=(valid_x, valid_y), callbacks=[earlystop], batch_size=32, nb_epoch=50)\n",
    "acc = evaluate(model, test_x, test_y)  #evaluate(model)\n",
    "\n",
    "labels = [\"air conditioner\", \"horn\", \"children\", \"dog\", \"drill\", \"engine\", \"gun\", \"hammer\", \"siren\", \"music\"]\n",
    "print(\"Showing Confusion Matrix\")\n",
    "y_prob = model.predict(test_x, verbose=0)\n",
    "y_pred = np.argmax(y_prob, axis=-1)\n",
    "y_true = np.argmax(test_y, 1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print(\"    \" + empty_cell, end=' ')\n",
    "    for label in labels:\n",
    "        print(\"%{0}s\".format(columnwidth) % label, end=' ')\n",
    "    print()\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print(\"    %{0}s\".format(columnwidth) % label1, end=' ')\n",
    "        for j in range(len(labels)):\n",
    "            cell = \"%{0}s\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print(cell, end=' ')\n",
    "        print()\n",
    "\n",
    "print_cm(cm, labels)\n",
    "\n",
    "df_cm = pd.DataFrame(cm, labels, labels)\n",
    "plt.figure(figsize=(16, 8))\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 14}, fmt='g', linewidths=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "\n",
    "print(\"History keys:\", (history.history.keys()))\n",
    "# summarise history for training and validation set accuracy\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "# summarise history for training and validation set loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "117px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
