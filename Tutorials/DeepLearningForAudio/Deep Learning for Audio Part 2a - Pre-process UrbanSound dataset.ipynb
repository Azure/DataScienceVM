{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep Learning for Audio Part 2a: pre-process UrbanSound Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook, we will process the audio files and extract the useful features that will be fed into a Convolutional Neural Network. \n",
    "\n",
    "\n",
    "\n",
    "We will train and predict on [UrbanSound8K](https://serv.cusp.nyu.edu/projects/urbansounddataset/download-urbansound8k.html) dataset. There are a few published benchmarks, which are mentioned in the papers below:\n",
    "\n",
    "- [Environmental sound classification with convolutional neural networks](http://karol.piczak.com/papers/Piczak2015-ESC-ConvNet.pdf) by Karol J Piczak.\n",
    "- [Deep convolutional neural networks and data augmentation for environmental sound classification](https://arxiv.org/abs/1608.04363) by Justin Salamon and Juan Pablo Bello\n",
    "- [Learning from Between-class Examples for Deep Sound Recognition](https://arxiv.org/abs/1711.10282) by Yuji Tokozume, Yoshitaka Ushiku, Tatsuya Harada\n",
    "\n",
    "\n",
    "The state-of-art result is from the last paper by Tokozume et al., where the best error rate achieved is 21.7%. In this tutorial we will show you how to build a neural network that can achieve the state-of-art performance using Azure.\n",
    "\n",
    "\n",
    "This jupyter notebook borrows some of the pre-processing code on the Github Repo here: http://aqibsaeed.github.io/2016-09-24-urban-sound-classification-part-2/, but with a lot of modifications. It is tested with **Python3.5**, **Keras 2.1.2** and **Tensorflow 1.4.0**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use librosa as our audio processing library. For more details on librosa, please refer to the librosa documenent [here](https://librosa.github.io/librosa/tutorial.html). We also need to install a bunch of libraries. Most of them are python packages, but you still may need to install a few audio processing libraries using apt-get:\n",
    "\n",
    "`sudo apt-get install -y --no-install-recommends \\\n",
    "        openmpi-bin \\\n",
    "        build-essential \\\n",
    "        autoconf \\\n",
    "        libtool \\\n",
    "        libav-tools \\\n",
    "        pkg-config`\n",
    "        \n",
    "        \n",
    "We also need to install librosa and a few other deep learning libraries in pip:\n",
    "\n",
    "`pip install librosa pydot graphviz keras tensorflow-gpu`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to licensing issues, we cannot download the data directly. Please go to the [UrbanSound8K Download](https://serv.cusp.nyu.edu/projects/urbansounddataset/download-urbansound8k.html) site, fill in the related information, download from there, and put it in the right place. You need to update the `parent_path` and `save_dir` below. In this particular case, we don't need the label file, as the labels are already reflected in the file names. We will parse the labels directly from the file names.\n",
    "\n",
    "PS: on Data Science Virtual Machine, you may want to run\n",
    "\n",
    "`\n",
    "sudo chown -R <your username> /mnt\n",
    "sudo chgrp -R <your username> /mnt\n",
    "`\n",
    " \n",
    "then move the downloaded UrbanSound8K.tar.gz dataset to `/mnt` and run\n",
    "\n",
    "`\n",
    "cd /mnt; tar xvzf UrbanSound8K.tar.gz\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and initialize global varaibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "# used to featurize the dataset\n",
    "from scipy import signal\n",
    "\n",
    "# how many classes do we have; for one-hot encoding and parallel processing purpose\n",
    "num_total_classes = 10\n",
    "\n",
    "# Where you have saved the UrbanSound8K data set. Need to be absolute path.\n",
    "parent_dir = \"/mnt/UrbanSound8K/audio\"\n",
    "\n",
    "# specify bands that you want to use. This is also the \"height\" of the spectrogram image\n",
    "n_bands = 150\n",
    "# specify frames that you want to use. This is also the \"width\" of the spectrogram image\n",
    "n_frames = 150\n",
    "\n",
    "\n",
    "# sample rate of the target files\n",
    "sample_rate = 22050\n",
    "# update this part to produce different images\n",
    "save_dir = \"/mnt/us8k-\" + str(n_bands) + \"bands-\" + str(n_frames) + \"frames-3channel\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the length of the sliding window used to featurize the data into a mel spectrogram is empirical â€“ based on [Environmental sound classification with convolutional neural networks](http://karol.piczak.com/papers/Piczak2015-ESC-ConvNet.pdf) paper by Piczak, longer windows seems to perform better than shorter windows. In this blog, we will use a sliding window with a length of 2s with a 1 second overlapping; this will also determines the width of our spectrogram. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read wav helper method to force audio resampling\n",
    "# duration is set for a 4 second clip\n",
    "def read_audio(audio_path, target_fs=None, duration=4):\n",
    "    (audio, fs) = librosa.load(audio_path, sr=None, duration=duration)\n",
    "    # if this is not a mono sounds file\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    if target_fs is not None and fs != target_fs:\n",
    "        audio = librosa.resample(audio, orig_sr=fs, target_sr=target_fs)\n",
    "        fs = target_fs\n",
    "    return audio, fs\n",
    "\n",
    "def pad_trunc_seq_rewrite(x, max_len):\n",
    "    \"\"\"Pad or truncate a sequence data to a fixed length.\n",
    "\n",
    "    Args:\n",
    "      x: ndarray, input sequence data.\n",
    "      max_len: integer, length of sequence to be padded or truncated.\n",
    "\n",
    "    Returns:\n",
    "      ndarray, Padded or truncated input sequence data.\n",
    "    \"\"\"\n",
    "\n",
    "    if x.shape[1] < max_len:\n",
    "        pad_shape = (x.shape[0], max_len - x.shape[1])\n",
    "        pad = np.ones(pad_shape) * np.log(1e-8)\n",
    "        #x_new = np.concatenate((x, pad), axis=1)\n",
    "        x_new = np.hstack((x, pad))\n",
    "    # no pad necessary - truncate\n",
    "    else:\n",
    "        x_new = x[:, 0:max_len]\n",
    "    return x_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(parent_dir, sub_dirs, bands, frames, file_ext=\"*.wav\"):\n",
    "    # 4 second clip with 50% window overlap with small offset to guarantee frames\n",
    "    n_window = int(sample_rate * 4. / frames * 2) - 4 * 2\n",
    "    # 50% overlap\n",
    "    n_overlap = int(n_window / 2.)\n",
    "    # Mel filter bank\n",
    "    melW = librosa.filters.mel(sr=sample_rate, n_fft=n_window, n_mels=bands, fmin=0., fmax=8000.)\n",
    "    # Hamming window\n",
    "    ham_win = np.hamming(n_window)\n",
    "    log_specgrams_list = []\n",
    "    labels = []\n",
    "    for l, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            # print(\"processing\", fn)\n",
    "            sound_clip, fn_fs = read_audio(fn, target_fs=sample_rate)\n",
    "            assert (int(fn_fs) == sample_rate)\n",
    "\n",
    "            if sound_clip.shape[0] < n_window:\n",
    "                print(\"File %s is shorter than window size - DISCARDING - look into making the window larger.\" % fn)\n",
    "                continue\n",
    "\n",
    "            label = fn.split('fold')[1].split('-')[1]\n",
    "            # Skip corrupted wavs\n",
    "            if sound_clip.shape[0] == 0:\n",
    "                print(\"File %s is corrupted!\" % fn)\n",
    "                continue\n",
    "                # raise NameError(\"Check filename - it's an empty sound clip.\")\n",
    "\n",
    "            # Compute spectrogram                \n",
    "            [f, t, x] = signal.spectral.spectrogram(\n",
    "                x=sound_clip,\n",
    "                window=ham_win,\n",
    "                nperseg=n_window,\n",
    "                noverlap=n_overlap,\n",
    "                detrend=False,\n",
    "                return_onesided=True,\n",
    "                mode='magnitude')\n",
    "            x = np.dot(x.T, melW.T)\n",
    "            x = np.log(x + 1e-8)\n",
    "            x = x.astype(np.float32).T\n",
    "            x = pad_trunc_seq_rewrite(x, frames)\n",
    "\n",
    "            log_specgrams_list.append(x)\n",
    "            labels.append(label)\n",
    "\n",
    "    log_specgrams = np.asarray(log_specgrams_list).reshape(len(log_specgrams_list), bands, frames, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis=3)\n",
    "    features = np.concatenate((features, np.zeros(np.shape(log_specgrams))), axis=3)\n",
    "    for i in range(len(features)):\n",
    "        # first order difference, computed over 9-step window\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "        # for using 3 dimensional array to use ResNet and other frameworks\n",
    "        features[i, :, :, 2] = librosa.feature.delta(features[i, :, :, 1])\n",
    "\n",
    "    return np.array(features), np.array(labels, dtype=np.int)\n",
    "\n",
    "# convert labels to one-hot encoding\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = num_total_classes\n",
    "    one_hot_encode = np.zeros((n_labels, n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Extracted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below can convert the raw audio files into features using multi-processing to fully utilize the CPU. The processed data are stored as numpy arrays and will be loaded during training time.\n",
    "\n",
    "It takes around 10 mins to complete - the time will vary depending on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 ms, sys: 52.8 ms, total: 81.5 ms\n",
      "Wall time: 9min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# use this to process the audio files into numpy arrays\n",
    "def save_folds(data_dir, k, bands, frames):\n",
    "    fold_name = 'fold' + str(k)\n",
    "    print(\"Saving \" + fold_name)\n",
    "\n",
    "    features, labels = extract_features(parent_dir, [fold_name], bands=bands, frames=frames)\n",
    "    labels = one_hot_encode(labels)\n",
    "\n",
    "    print(\"Features of\", fold_name, \" = \", features.shape)\n",
    "    print(\"Labels of\", fold_name, \" = \", labels.shape)\n",
    "\n",
    "    feature_file = os.path.join(data_dir, fold_name + '_x.npy')\n",
    "    labels_file = os.path.join(data_dir, fold_name + '_y.npy')\n",
    "    np.save(feature_file, features)\n",
    "    print(\"Saved \" + feature_file)\n",
    "    np.save(labels_file, labels)\n",
    "    print(\"Saved \" + labels_file)\n",
    "\n",
    "\n",
    "def assure_path_exists(path):\n",
    "    mydir = os.path.join(os.getcwd(), path)\n",
    "    if not os.path.exists(mydir):\n",
    "        os.makedirs(mydir)\n",
    "\n",
    "\n",
    "assure_path_exists(save_dir)\n",
    "Parallel(n_jobs=num_total_classes)(delayed(save_folds)(save_dir, k, bands=n_bands, frames=n_frames) for k in range(1, 11))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "toc": {
   "nav_menu": {
    "height": "171px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
