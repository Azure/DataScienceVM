{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing IoT data \n",
    "\n",
    "## Summary \n",
    "\n",
    "This notebook explains how to process telemetry data coming from IoT devices that arrives trough a gateway enabled edgeHub.\n",
    "\n",
    "## Description\n",
    "\n",
    "The purpose of this notebook is to explain and guide the reader onto how to process telemetry data generated from IoT devices whitin the DSVM IoT extension.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "* A gateway enabled Edge Runtime. See 'Setting up IoT Edge'\n",
    "* A sniffer architecture deployed. See 'Obtaining IoT Telemetry'\n",
    "* A device sending telemetry to your gateway. for this notebook we have choosed the scenario where a device is sending Temperature telemetry.\n",
    "\n",
    "## Documentation\n",
    "\n",
    "* https://tutorials-raspberrypi.com/raspberry-pi-measure-humidity-temperature-dht11-dht22/\n",
    "* http://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Reading generated data\n",
    "\n",
    "During this step we are going to load the data generated from the IoT devices. The sniffer module mounts a docker volume in order to share data between the module and the host, in order to retrieve the path where the module is storing it's data you can run the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Listing the volumes\n",
    "sudo docker volume ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Getting the volume path\n",
    "sudo docker inspect <volume id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the path that was obtained as result from the last command.\n",
    "\n",
    "Since the file location is protected, we are going to make a directory and copy the file over there. The name of the file generated from the module is called data.json\n",
    "\n",
    "Path: volume_path/data.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "## Making a directory\n",
    "mkdir \"/home/$USER/IoT/Data\"\n",
    "sudo cp <file path> \"/home/$USER/IoT/Data/data.json\"\n",
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to extract the data using python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "## Reading the data from the file\n",
    "## Note Change your user\n",
    "path = \"/home/<user>/IoT/Data\"\n",
    "file = path + \"/data.json\"\n",
    "data = {}\n",
    "with open(file) as f:\n",
    "    for line in f.readlines():\n",
    "        sample = json.loads(line)\n",
    "        for key in sample.keys():\n",
    "            if key in data:\n",
    "                data[key].append([len(data[key]),sample[key]])\n",
    "            else:\n",
    "                data[key] = []\n",
    "                data[key].append([len(data[key]),sample[key]])\n",
    "\n",
    "temperature = np.array(data['temperature'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Using a low pass filter in order to detect anomalies \n",
    "\n",
    "\"The simplest approach to identifying irregularities in data is to flag the data points that deviate from common statistical properties of a distribution, including mean, median, mode, and quantiles. Let's say the definition of an anomalous data point is one that deviates by a certain standard deviation from the mean. Traversing mean over time-series data isn't exactly trivial, as it's not static. You would need a rolling window to compute the average across the data points. Technically, this is called a rolling average or a moving average, and it's intended to smooth short-term fluctuations and highlight long-term ones. Mathematically, an n-period simple moving average can also be defined as a 'low pass filter.' \"\n",
    "\n",
    "\n",
    "In this next step we are going to build a low pass filter (moving average) using discrete linear convolution to detect anomalies in our telemetry data. Check the documentation for a more detailed explanation of the theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import  count\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linspace, loadtxt, ones, convolve\n",
    "import pandas as pd\n",
    "import collections\n",
    "from random import randint\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "print(temperature)\n",
    "## Adding some noise\n",
    "#temperature[50][1] = 50.0\n",
    "#temperature[100][1] = 75.0\n",
    "#temperature[150][1] = 50.0\n",
    "#temperature[200][1] = 75.0\n",
    "data_as_frame = pd.DataFrame(temperature, columns=['index','temperature'])\n",
    "data_as_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes moving average using discrete linear convolution of two one dimensional sequences.\n",
    "def moving_average(data, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(data, window, 'same')\n",
    "\n",
    "# Helps in exploring the anamolies using stationary standard deviation\n",
    "def explain_anomalies(y, window_size, sigma=1.0):\n",
    "    avg = moving_average(y, window_size).tolist()\n",
    "    residual = y - avg\n",
    "    # Calculate the variation in the distribution of the residual\n",
    "    std = np.std(residual)\n",
    "    return {'standard_deviation': round(std, 3),\n",
    "            'anomalies_dict': collections.OrderedDict([(index, y_i) for\n",
    "                                                       index, y_i, avg_i in zip(count(), y, avg)\n",
    "              if (y_i > avg_i + (sigma*std)) | (y_i < avg_i - (sigma*std))])}\n",
    "\n",
    "# Helps in exploring the anamolies using rolling standard deviation\n",
    "def explain_anomalies_rolling_std(y, window_size, sigma=1.0):\n",
    "    avg = moving_average(y, window_size)\n",
    "    avg_list = avg.tolist()\n",
    "    residual = y - avg\n",
    "    # Calculate the variation in the distribution of the residual\n",
    "    testing_std = pd.rolling_std(residual, window_size)\n",
    "    testing_std_as_df = pd.DataFrame(testing_std)\n",
    "    rolling_std = testing_std_as_df.replace(np.nan,\n",
    "                                  testing_std_as_df.ix[window_size - 1]).round(3).iloc[:,0].tolist()\n",
    "    std = np.std(residual)\n",
    "    return {'stationary standard_deviation': round(std, 3),\n",
    "            'anomalies_dict': collections.OrderedDict([(index, y_i)\n",
    "                                                       for index, y_i, avg_i, rs_i in zip(count(),\n",
    "                                                                                           y, avg_list, rolling_std)\n",
    "              if (y_i > avg_i + (sigma * rs_i)) | (y_i < avg_i - (sigma * rs_i))])}\n",
    "\n",
    "\n",
    "# This function is repsonsible for displaying how the function performs on the given dataset.\n",
    "def plot_results(x, y, window_size, sigma_value=1,text_xlabel=\"X Axis\", text_ylabel=\"Y Axis\", applying_rolling_std=False):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(x, y, \"k.\")\n",
    "    y_av = moving_average(y, window_size)\n",
    "    plt.plot(x, y_av, color='green')\n",
    "    plt.xlim(0, 1000)\n",
    "    plt.xlabel(text_xlabel)\n",
    "    plt.ylabel(text_ylabel)\n",
    "\n",
    "    # Query for the anomalies and plot the same\n",
    "    events = {}\n",
    "    if applying_rolling_std:\n",
    "        events = explain_anomalies_rolling_std(y, window_size=window_size, sigma=sigma_value)\n",
    "    else:\n",
    "        events = explain_anomalies(y, window_size=window_size, sigma=sigma_value)\n",
    "\n",
    "    x_anomaly = np.fromiter(events['anomalies_dict'].keys(), dtype=int, count=len(events['anomalies_dict']))\n",
    "    y_anomaly = np.fromiter(events['anomalies_dict'].values(), dtype=float,\n",
    "                                            count=len(events['anomalies_dict']))\n",
    "    plt.plot(x_anomaly, y_anomaly, \"r*\", markersize=12)\n",
    "\n",
    "    # add grid and lines and enable the plot\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_as_frame['index']\n",
    "Y = data_as_frame['temperature']\n",
    "\n",
    "\n",
    "# plot the results\n",
    "plot_results(x, y=Y, window_size=10, text_xlabel=\"Moment\", sigma_value=3,\n",
    "             text_ylabel=\"Temperature\")\n",
    "events = explain_anomalies(y, window_size=5, sigma=3)\n",
    "\n",
    "# Display the anomaly dict\n",
    "print(\"Information about the anomalies model:{}\".format(events))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
